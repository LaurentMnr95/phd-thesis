\section{Introduction to Adversarial Classification}

In this section, we present the required background about adversarial classification. In the first part, we present formally what is an adversarial attack, then how to craft them in practice. After, we present ways of defending against adversarial examples. Finally, we state the main results about theoretical understanding of adversarial examples. 
\subsection{What is an adversarial example?}

In classification tasks, an adversarial example is a perturbation of an input that is imperceptible to humans, but that state-of-the-art classifiers are unable to  classify accurately. In the following of the manuscript we define adversarial attacks as follows.


\begin{definition} 
    Let $h:\XX\to\YY$ be a classifier. An adversarial attack of level $\varepsilon$ on the input $x$ with label $y$ against the classifier $h$ is a perturbation $x'$ such that:
    \begin{align*}
        h(x')\neq y\quad\text{and}\quad d(x,x')\leq\varepsilon~.
    \end{align*}
\end{definition}
This definition is very simple and general. The distance $d$ can refer to an $\ell^p$ distance, taken as a surrogate to a perception distance. We can associate to adversarial examples a notion of adversarial risk. The adversarial risk is the worst case risk if each point is optimally attacked at level $\varepsilon$.
\begin{definition}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. Let $h:\XX\to\YY$ be a classifier. We define the \emph{adversarial risk} of $h$ at level $\varepsilon$ as:
  \begin{align*}
    \risk_\varepsilon(h):=\PP\left[\exists x'\in B_\varepsilon(x),~h(x')\neq y\right] = \mathbb{E}_{(x,y)\sim\PP}\left[\sup_{x'\in B_\varepsilon(x)}\mathbf{1}_{h(x')\neq y}\right]
  \end{align*}

  where $B_\varepsilon(x) = \{x'\in\XX\mid~d(x,x')\leq\varepsilon\}$. If $f:\XX\to\RR^K$, then the adversarial risk of $f$ at level $\varepsilon$ is defined as
   \begin{align*}
    \risk_{\PP}(f):=\PP\left[\exists x'\in B_\varepsilon(x),~\argmaxB_{k\in\YY}f_k(x')\neq y\right]
   \end{align*}
\end{definition}
A first property is that the adversarial risk is well defined. While this result seems trivial, it requires advanced arguments from measure theory. 
\begin{prop}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. Let $h:\XX\to\YY$ be a classifier. If $h$ is Borel measurable then $(x,y)\mapsto \sup_{x'\in B_\varepsilon(x)}\mathbf{1}_{h(x')\neq y}$ is universally measurable.
\end{prop}
\begin{proof}
  We define $\phi_\varepsilon(x,y,f) = \sup_{x'\in B_\varepsilon(x)}\mathbf{1}_{h(x)\neq y}$.  We have :
  \begin{align*}
     \phi_\varepsilon(x,y,f) =  \sup_{(x',y')\in\XX\times\YY}\mathbf{1}_{h(x')\neq y'}-\infty\times \mathbf{1}\{d(x',x)\geq \varepsilon\text{ or }y'\neq y\}
  \end{align*}
  Then,
  \begin{align*}
      \left((x,y),(x',y')\right)\mapsto\mathbf{1}_{h(x')\neq y'}-\infty\times \mathbf{1}\{d(x',x)\geq \varepsilon\text{ or }y'\neq y\}
  \end{align*}
  defines a measurable, hence upper semi-analytic function. Using~\citep[Proposition 7.39, Corollary 7.42]{bertsekas2004stochastic}, we get that for all $f\in\mathcal{F}(\XX)$, $(x,y)\mapsto\phi_\varepsilon(x,y,f)$ is a universally measurable function.
\end{proof}

Similarly to the standard classification setting, we define the optimal bayes risk for adversarial classification. 

\begin{definition}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. We call \emph{adversarial Optimal Bayes risk} of level $\varepsilon$ , the infimum of adversarial risk of level $\varepsilon$  over the set of Borel measurable classifiers $\mathcal{F}(\XX,\YY)$:
\begin{align*}
  \risk^\star_\varepsilon:=\inf_{h\in\mathcal{F}(\XX,\YY)} \risk_\varepsilon(h)
\end{align*}

\end{definition}

Contrarily to the the standard case, the existence of optimal Bayes classifiers for the adversarial risk is a difficult question.


\subsection{Casting Adversarial examples}

The probably most puzzling about adversarial examples is the facility to craft them. Let us consider an attacker that aim at finding an adversarial perturbation $x'$ of an input $x$ for a given classifier $\mathbf{f}$.  In order to craft an adversarial example, typically the cross-entropy, the attacker maximizes the following objective given a differentiable loss $\loss$:
\begin{align}
    \max_{x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon}\loss\left(\mathbf{f}(x'), y)\right).
\end{align}
In this case the attack is said to be \emph{untargeted}, i.e. the classifier aims at evading the label $y$. On the other side, a \emph{targeted attack} aims at perturbing a label $x$ to make it classify to a target label $y$. In this case, the attacker objective writes: $\min_{x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon}\loss\left(\mathbf{f}(x'), y)\right)$. An attacker may also target at finding the smallest perturbation problem~\citep{moosavi2016deepfool,carlini2017towards}.  Many attacks were proposed that we will categorize into two parts: white-box attacks and black-box attacks.
\paragraph{White box attacks:} In this setting, the attacker has  full knowledge of the function $\mathbf{f}$ and its parameters. Hence, these attacks often takes advantages of the differentiablity of $\mathcal{f}$ and the loss function $\loss$. Then, such attacks usually takes the gradient $\nabla_x \loss\left(f\left(x^t\right),y\right)$ as ascent direction for crafting adversarial examples.  These attacks are called \emph{gradient based attacks}. The most popular white box attacks are PGD attack~\cite{kurakin2016adversarial,madry2018towards}, FGSM attack~\citep{goodfellow2014explaining}, Carlini\&Wagner attack~\citep{carlini2017towards}, AutoPGD~\citep{Croce2020MinimallyDA}, FAB~\citep{Croce2020MinimallyDA}, etc. As an illustration of the simplicity of crafting adversarial examples, we show hereafter how the desingn of a PGD attack in an $\ell_p$ case.
\begin{example*}[PGD attack] Let $x_0\in\RR^d$ be an input. The projected gradient descent (PGD)~\cite{kurakin2016adversarial,madry2018towards} of radius~$\varepsilon$, recursively computes
\begin{align*}
x^{t+1}=\prod_{B_p(x,\varepsilon)}\left(x^t
+\alpha \argmaxB_{\delta\text{ s.t. }||\delta||_p\leq1} \langle\Delta^t,\delta \rangle\right)
\end{align*}
where $B_p(x,\varepsilon) = \{ x+\tau \text{~s.t.~} \lVert\tau\rVert_p \leq \epsilon\}$, $\Delta^t=\nabla_x \loss\left(f\left(x^t\right),y\right)$, $\alpha$ is a gradient step size, and $\prod_S$ is the orthogonal projection operator on $S$. Many attacks are extensions of this one as AutoPGD~\citep{croce2020reliable} and SparsePGD~\citep{tramer2019adversarial}
\end{example*}
\paragraph{Black box attacks:} In this setting, the attacker has limited knowledge of the classifier. The attacker does not have have access to the parameters of the classifier, but can query either the predicted logits or the predicted label for a given input $x$. To craft adversarial examples, it was proposed to mimic gradient-based attacks using gradient estimation as the ZOO attack~\citep{chen2017zoo} and NES attack~\citep{ilyas2018black,ilyas2019adversarial}. Attacks might also be based on other optimization methods such as combinatorial methods~\citep{moon19aparsimonous} or evolutionary computation~\citep{andriushchenko2019square}.










\paragraph{Adversarial Examples beyond Image Classification.} Adversarial examples do not only exist in Image Classification, although it is the most spectacular example as images are perceptually unchanged. We can enumerate, non exhaustively, the following examples of adversarial classification:
\begin{itemize}
    \item \textbf{Image Segmentation and Object Detection:} \cite{xie2017adversarial} proposed to attack image segmentation and object detection. The goal of such attack is enforce a undesirable detection or segmentation in an image. 
    \item \textbf{Video classification:} Videos are series of images. Adversarial attacks against video classification systems are closed to adversarial examples in standard Image Classification. Adversarial attacks might aim at changing either a bit many frames~\citep{jiang2019black} or a lot only a few frames~\citep{mu2021sparse}. 
    \item \textbf{Audio systems:} Audio systems can be fooled by adding inaudible adversarial noise to an audio file~\citep{carlini2018audio}. These attacks raise issues in the massive use of personal vocal assistants~\citep{zhang2019dangerous}. 
    \item \textbf{NLP classification tasks:} Adversaries change some words in a text to make it misclassified. However such examples can also change the meaning of the text and consequently change its classification also humans. Examples of attempts for adversarial examples against NLP systems can be either black box~\citep{jin2019bert,li2020bert} or gradient-based~\citep{guo2021gradient}
    \item \textbf{Recommender Systems} A recent line of work~\cite{jun2018adversarial,liu2019data,garcelon2020adversarial} aimed at crafting adversarial attacks against bandit algorithms~\citep{lattimore2018bandit}. The goal of these attacks are to force the learner to chose the wrong arms a linear number of times. While these works are mostly theoretical, their potential use in practical settings might raise issues for businesses in a close future.
\end{itemize}



% \subsection{A Learning Approach to Adversarial Classification}
% Adversarial classification consists in  learning a (universally or Borel) measurable function $h:\X\to\YY$ minimizing the $0/1$ loss risk: 
% \begin{align*}
% \risk^\varepsilon_{0/1}(h)&:=\PP_{(x,y)}(\exists x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon \text{ and } h(x')\neq y)\\
% &= \EE_{(x,y)\sim\PP}\left[\sup_{x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon}\mathbf{1}_{h(x')\neq y}\right]
% \end{align*}
% The definition of this quantity is not immediate and requires the following proposition.

% \begin{prop}
% For any  Borel measurable function $h$, the adversarial risk is well defined $\risk^\varepsilon_{0/1}(h)$.
% \end{prop}
% TBC
% The existence of a minimizer for adversarial risk is a difficult question, that was partially answered in NOTYETPUBLISHED, which states, under some mild conditions, that the minimum is attained over the set of universally measurable functions.


%  define general adversarial loss.



% The question of the loss function 
\subsection{Defending against adversarial examples}

Defending against adversarial examples is still an open research questions with few answers to it. One can derive the methods in two categories: empirical defenses and provable defenses.

\paragraph{Provable defenses.} A defense is said to be provable if there is a theoretical guarantee to ensure a level of robustness. Formally, a classifier $h$ is said to \emph{certifiably robust at level $\varepsilon$} at input $x$ with label $y$ if there exist no adversarial example of level $\varepsilon$ on $h$ at the point $(x,y)$, i.e. for all $x'$ such that $d(x,x')\leq\varepsilon$, $h(x') = y$. Researchers have focused on finding ways to certify robustness. The first categories of defenses relies on convex relaxation of layers~\citep{wong2018provable,wong2018scaling}. It consists to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation of an input. In the case of ReLU activation, the robust optimization problem  that minimizes the worst case loss over this outer region writes as a linear program. Another developed method is noise injection to the input~\citep{lecuyer2019certified,KolterRandomizedSmoothing,pinot2019theoretical,salman2019provably}. By adding a noise, the inputs can be seen as distributions. The certificates are derived by determining which classifier would be the most powerful to distinguish two inputs. This idea is closely related to the notions of statistical tests~\citep{KolterRandomizedSmoothing}, information theory~\citep{pinot2019theoretical} and differential privacy~\citep{lecuyer2018certified}. Finally, a last trend to develop provably robust neural networks is to enforce Lipschitzness property~\citep{tsuzuku2018lipschitz}. Many papers hav worked on designing Lipschitz layers~\citep{li2019preventing,trockman2021orthogonalizing,skew2021sahil} and activations~\citep{anil2019sorting,singla2021householder,huang2021local}.


\begin{algorithm}[h!]
  \SetAlgoLined
  $T$: number of iterations, Level of attack $\varepsilon$ \\
   \For{$t=1,\dots,T$}{
   Let $B_t$ be a batch of data.\\
  $\Tilde{B}_t\leftarrow$ Attack of level $\varepsilon$ on images in $B_t$ for the  model $f_{\theta_t}$ (using PGD for instance)\\
  $\theta^t_k\leftarrow$ Update $\theta^{t-1}_k$ with $\Tilde{B}_t$ with a SGD or Adam step
  }
   \caption{Adversarial Training algorithm}
   \label{algo:adv-training}
\end{algorithm}
  

\paragraph{Empirical defenses.} Defenses against adversarial examples often have no theoretical guarantees and based on training heuristics. The first defense that was proposed is \emph{Adversarial Training}~\citep{goodfellow2014explaining,madry2018towards}. This defense is an heuristic to minimize the adversarial risk. We describe the adversarial training defense in Algorithm~\ref{algo:adv-training} to training a classifier $f_\theta$ parametrized by $\theta$. It consists minimization steps and attacks on the classifier to make it more robust. To our knowledge there exists no proof of convergence for this defense. Many other empirical defenses are variants of Adversarial Training as TRADES~\citep{zhang2019theoretically} or MART~\citep{wang2019improving}. For instance, TRADES aims at minimizing the following objective:
\begin{align*}
  f\mapsto\mathbb{E}\left[L(f(x),y)+\lambda\times\max_{x'\in B_\varepsilon(x)}L(f(x'),f(x))\right]\quad.
\end{align*}
The first term aims at optimizing standard robustness and the second term is a regularization for adversarial robustness. The objective is to better balance the tradeoff between robustness and standard accuracy. Similarly to Adversarial Training, the inner supremum is optimized using PGD algorithm.

Another promising way to defend against adversarial examples is to augment the dataset. For instance,~\cite{carmon2019unlabeled,rebuffi2021fixing} proposed to use unlabeled data to improve Adversarial Training strategies. Other works as~\citep{wang2019improving} proposed to use artificially generated inputs to improve adversarial robustness. We do not enter into details of these but most powerful defenses uses one of these techniques~\citep{croce2020robustbench}.



\paragraph{Evaluation Protocol.}

Unless the used defense mechanisms are provable and provide guarantees, evaluating and assessing adversarial robustness is rigorous and meticulous task for empirical defenses. For instance, many papers introduced ``defenses'' that were actually proven to be ``false''~\citep{athalye2018obfuscated,carlini2019evaluating}. Indeed, when proposing a defense, one needs to adapt the attack model to the defense. We describe the following common issues. For instance, when evaluating against randomized classifiers in either white-box or black-box setting, the return output is a random variable, hence the computation of an attack against it needs to be adapted to the non-deterministic nature of the classifier. To do so, \cite{athalye2018obfuscated} proposed to average either the logits or the gradient of the classifier to build a suitable attack against a randomized classifier. This procedure was called Expectation Over Transformation (EOT). A second example is defenses that aims at using non differentiable activation functions as Heaviside functions.~\cite{athalye2017synthesizing} proposed to use BPDA (xxx), i.e. differentiable approximations to circumvent the ``defense''. Black-box attacks are also a way to build efficient attacks in this case. 

To answer the need of adversarial examples research community to evaluate accurately their models against adversarial examples, \cite{croce2020robustbench} proposed RobustBench as a unified platform for benchmarking adversarial defenses. The platform evaluates models on different black-box and white-box, targeted and untargeted attacks (AutoPGD~\citep{croce2020reliable}, FAB~\citep{Croce2020MinimallyDA}, SquareAttack~\citep{andriushchenko2019square}). However, this platform has its limitations:  for instance, it  does not propose to evaluate the robustness of randomized classifiers. 

\paragraph{State-of-the-art in Image Classification}

To evaluate the perfomance of an attack of a classification algorithm, one needs to train and evaluate on datasets. In image classification evaluation, three datasets are mainly used:
\begin{itemize}
    \item \textbf{MNIST~\citep{lecun1998mnist}:} A dataset of black and white low-quality images representing the $10$ digits. The training set contains $50000$ images and test set $10000$ images. These images are of dimension $28\times28\times 1$ ($784$ in total). This dataset is known to be easy ($>99\%$ can be obtained using simple classifiers). In adversarial classification, the problem is also easy to be solved. Evaluation on MNIST is not sufficient to assess the performance of a classifier or even a defense against adversarial examples.
    \item \textbf{CIFAR10 and CIFAR100~\citep{krizhevsky2009learning}:} Datasets of colored low-quality images representing the $10$ labels and $100$ labels for respectively CIFAR10 and CIFAR100. Each training set contains $50000$ images and test set $10000$ images. These images are of dimension $32\times32\times 1$ ($3072$ in total). The current state-of-the-art on CIFAR10 in standard classification is $>99\%$ of accuracy, but asks advanced methods to reach such a score. On CIFAR100, the current state-of-the-art is around $94\%$. In adversarial classification both datasets are challenging and difficult. The evolution of state-of-the-art in adversarial classification is available in RobustBench\footnote{\url{https://robustbench.github.io/}}. Benchmark in adversarial classification are often made on these datasets.
    \item \textbf{ImageNet~\citep{imagenet_cvpr09}:} ImageNet refers to a dataset containing $1.2$ million of images labeled into $1000$ classes. Images are of diverse qualities, but models often takes input of dimension $224\times224\times 3$ (dimension $150528$ in total). The current state-of-the-art on ImageNet is about $87\%$. There is no need to say that adversarial classification on ImageNet is still a very-challenging task. Further than the standard dataset, ImageNet project is still in development: the project gathers $14197122$ images and $21841$ labels on August 31th, 2021.   

\end{itemize}




\subsection{Theoretical knowledge in Adversarial classification}


\paragraph{Curse of dimensionality.} From the seminal paper on adversarial examples on deep learning systems~\citep{Szegedy2013IntriguingPO}, the input dimension has been considered as an argument for inevitability of adversarial attacks. To assess this intuition,~\citet{gilmer2018adversarial,shafahi2018adversarial} proved that for a wide range of distribution $\PP$ on the unit sphere of dimension $D$, and any classifier $h$ it is possible to find an attack on examples $x$ with high probability, exponentially depending on the dimension of $\XX$. The arguments relies on isoperimetric inequalities and was extended to log-concave distributions on Riemannian manifolds and uniform distribution over positively curved Riemannian manifolds~\citep{pmlr-v97-dohmatob19a}. 

\citep{simon2019first} also tried to explain the exitence of adversarial examples for neural networks under the light of the high dimensionality of inputs. The authors assumed that neetworks have ReLU activations and that the distributions of weight are Gaussian. Under such hypothesis, they proved that the gradient norm with regards the input is highly dependent on the dimension of the input, then justifying again that the dimensionality of the input is a reason for existence of adversarial examples. 




\paragraph{Generalization Bounds in Adversarial Learning.} Similarly to the standard classification case, research have focused on computing uniform bounds for adversarial classification. These works are often inspired from generalizations of standard tools as VC-dimension~\citep{cullina2018pac} or Rademacher complexity~\citep{yin2019rademacher,khim2018adversarial,awasthi2020adversarial} is the adversarial case. They exhibit generalization bounds that are highly dependent on the dimension of the input. Indeed the Rademacher complexity for classes adapted to the adversarial case add a polynomial term in the dimension $D$ of the input. However, for randomized classifiers, it is difficult to adapt PAC-Bayes bounds to the adversarial setting~\citep{viallard2021pac}. Indeed, the proof schemes cannot be used in the adversarial setting. Moreover, there is still misunderstanding in the bias-complexity tradeoff in the adversarial case~\citep{wang2018analyzing}. 

\paragraph{Adversarial Bayes Risk.} The adversarial bayes risk has been studied only very recently by researchers.~\cite{bhagoji2019lower,pydi2019adversarial,trillos2020adversarial} expressed the adversarial risk as an optimal transport problem for a suitable cost. Another approach was to study the adversarial risk from a game theoretic perspective. We will explain in details these contributions in Section~\ref{sec:otadv}. 


One of the recent contributions is the existence of  optimal classifiers for the adversarial setting.  The problem is not trivial because of the inner supremum and the difficulty to define a suitable topology on the space of measurable functions. The two papers~\citep{awasthi2021existence,bungert2021geometry} propose two different approaches for proving the existence of Bayes classifiers.~\cite{bungert2021geometry} proposed a $L^1+TV$ decomposition~\citep{chan2005aspects} of the adversarial risk.  To this end, the authors introduced a non-local perimeter satisfying the submodularity property. They got interested in a suitable relaxation of the adversarial with $\nu$ essential supremum where $\nu$ is a well-chosen distribution. This allows to study the problem in $L^\infty(\mathcal{X},\nu)$. The properties of this relaxation are nice (i.e. compactness and semi-continuity) which allows the authors to prove the existence of a minimizer for the relaxed problem. From this solution, the authors build a solution to the the adversarial problem that is Borel-measurable. The authors studied the regularity properties of these minimizers.

 