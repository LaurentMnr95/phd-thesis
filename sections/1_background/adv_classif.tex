
\section{Introduction to Adversarial Classification}

In this section, we present the required background about adversarial classification. In the first part, we present formally what is an adversarial attack, then how to craft them in practice. After, we present ways of defending against adversarial examples. Finally, we state the main results about theoretical understanding of adversarial examples. 
\subsection{What is an adversarial example?}

In classification tasks, an adversarial example is a perturbation of an input that is imperceptible to human senses, but that state-of-the-art classifiers are unable to  classify accurately. In the following of the manuscript we define adversarial attacks as follows.


\begin{definition} 
    Let $h:\XX\to\YY$ be a classifier. An adversarial attack of level $\varepsilon$ on the input $x$ with label $y$ against the classifier $h$ is a perturbation $x'$ such that:
    \begin{align*}
        h(x')\neq y\quad\text{and}\quad d(x,x')\leq\varepsilon~.
    \end{align*}
\end{definition}
This definition is very simple and general. The distance $d$ can refer to either a $l^p$ distance or a perception distance. We can associate to adversarial examples a notion of adversarial risk. The adversarial risk is the worst case risk if each point are optimally attacked at level $\varepsilon$.
\begin{definition}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. Let $h:\XX\to\YY$ be a classifier. We define the \emph{adversarial risk} of $h$ at level $\varepsilon$ as:
  \begin{align*}
    \risk_\varepsilon(h):=\PP\left[\exists x'\in B_\varepsilon(x),~h(x')\neq y\right] = \mathbb{E}_{(x,y)\sim\PP}\left[\sup_{x'\in B_\varepsilon(x)}\mathbf{1}_{h(x')\neq y}\right]
  \end{align*}

  where $B_\varepsilon(x) = \{x'\mid|~d(x,x')\leq\varepsilon\}$. If $f:\XX\to\RR^K$, then the adversarial risk of $f$ at level $\varepsilon$ is defined as
   \begin{align*}
    \risk_{0/1,\PP}(f):=\PP\left[\exists x'\in B_\varepsilon(x),~\argmaxB_{k\in\YY}f_k(x)\neq y\right]
   \end{align*}
\end{definition}
A first property is that the adversarial risk is well defined. While this result seems trivial, it requires advanced arguments from measure theory. 
\begin{prop}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. Let $h:\XX\to\YY$ be a classifier. If $h$ is Borel measurable then $\risk_\varepsilon(h)$ is well defined.
\end{prop}
\begin{proof}
  TODO
\end{proof}

Similarly to the standard classification setting, 

\begin{definition}
  Let $\PP$ be a Borel distribution over $\XX \times \YY$. We call \emph{adversarial Optimal Bayes risk}at level $\varepsilon$ , the infimum of adversarial risk at level $\varepsilon$  over the set of Borel measurable classifiers $\mathcal{F}(\XX,\YY)$:
\begin{align*}
  \risk^\star_\varepsilon:=\inf_{h\in\mathcal{F}(\XX,\YY)} \risk_\varepsilon(h)
\end{align*}

\end{definition}


\subsection{Adversarial examples in the wild}

The probably most puzzling about adversarial examples is the facility to craft them. Let us consider an attacker that aim at finding an adversarial perturbation $x'$ of an input $x$ for a given classifier $\mathbf{f}$.  In order to craft an adversarial example, typically the cross-entropy, the attacker maximizes the following objective given a differentiable loss $\loss$:
\begin{align}
    \max_{x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon}\loss\left(\mathbf{f}(x'), y)\right).
\end{align}
To solve the previous optimization problem, many attacks were proposed that we will categorize into two parts: white-box attacks and black-box attacks.
\paragraph{White box attacks:} In this setting, the attacker has  full knowledge of the function $\mathbf{f}$ and its parameters. Hence, these attacks often takes advantages of the differentiablity of $\mathcal{f}$ and the loss function $\loss$.Then, such attacks usually takes the gradient $\nabla_x \loss\left(f\left(x^t\right),y\right)$ as ascent direction for crafting adversarial examples.  These attacks are called \emph{gradient based attacks}. The most popular white box attacks are PGD attack~\cite{kurakin2016adversarial,madry2017towards}, FGSM attack~\citep{goodfellow2014explaining}, Carlini\&Wagner attack~\citep{carlini2017towards}, AutoPGD~\citep{Croce2020MinimallyDA}, FAB~\citep{Croce2020MinimallyDA}, etc. As an illustration of the simplicity of crafting adversarial examples, we show hereafter how the desingn of a PGD attack in an $\ell_p$ case.
\begin{example*}[PGD attack] Let $x_0\in\RR^d$ be an input. The projected gradient descent (PGD)~\cite{kurakin2016adversarial,madry2017towards} of radius~$\varepsilon$, recursively computes
\begin{align*}
x^{t+1}=\prod_{B_p(x,\varepsilon)}\left(x^t
+\alpha \argmaxB_{\delta\text{ s.t. }||\delta||_p\leq1} \langle\Delta^t,\delta \rangle\right)
\end{align*}
where $B_p(x,\varepsilon) = \{ x+\tau \text{~s.t.~} \lVert\tau\rVert_p \leq \epsilon\}$, $\Delta^t=\nabla_x \loss\left(f\left(x^t\right),y\right)$, $\alpha$ is a gradient step size, and $\prod_S$ is the orthogonal projection operator on $S$. Many attacks are extensions of this one as AutoPGD~\citep{Croce2020ReliableEO} and SparsePGD~\citep{tramer2019adversarial}
\end{example*}
\paragraph{Black box attacks:} In this setting, the attacker has limited knowledge of the classifier. The attacker does not have have access to the parameters of the classifier, but can query either the predicted logits or the predicted label for a given input $x$. To craft adversarial examples, it was proposed to mimic gradient-based attacks using gradient estimation as the ZOO attack~\citep{xxx} and NES attack~\citep{ilyas2018black,ilyas2019adversarial}. 



of the attacker has no knowledge on the classifier parameters and a limited access to the classifier, e.g. he can only access logits or predicted class for instance. CITE ZOO, nes. square, yetanother

TO COMPLETE







\paragraph{Adversarial Examples beyond Image Classification.} Adversarial examples do not only exist in Image Classification, but it is the most striking example as images are perceptually unchanged. We can enumerate, non exhaustively, the following examples of adversarial classification:
\begin{itemize}
    \item \textbf{Image Segmentation and Object Detection:} \cite{xie2017adversarial} proposed to attack image segmentation and object detection. The goal of such attack is enforce a undesirable detection or segmentation in an image. 
    \item \textbf{Video classification:} Videos are series of images. Adversarial attacks against video classification systems are closed to adversarial examples in standard Image Classification. Adversarial attacks might aim at changing either a bit many frame or a lot only a few frames. For instance,~\cite{}
    \item \textbf{Audio systems:} Audio systems can be fooled by adding inaudible adversarial noise to an audiofile. 
    \item \textbf{NLP classification tasks:} Adversaries change some words in a text to make it misclassified. However such examples can also change the meaning of the text and consequently change its classification also humans. Examples of attempts for adversarial examples against NLP systems are
    \item \textbf{Recommender Systems} A recent line of work~\cite{xxx,garcelon2020adversarial} aimed at crafting adversarial attacks against bandit algorithms~\citep{lattimore2018bandit}. The goal of these attacks are to force the learner to chose the wrong arms a linear number of times. While these works are mostly theoretical, their potential use in practical settings might raise issues for businesses in a close future.
\end{itemize}



% \subsection{A Learning Approach to Adversarial Classification}
% Adversarial classification consists in  learning a (universally or Borel) measurable function $h:\X\to\YY$ minimizing the $0/1$ loss risk: 
% \begin{align*}
% \risk^\varepsilon_{0/1}(h)&:=\PP_{(x,y)}(\exists x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon \text{ and } h(x')\neq y)\\
% &= \EE_{(x,y)\sim\PP}\left[\sup_{x'\in\XX\text{ s.t. } d(x,x')\leq \varepsilon}\mathbf{1}_{h(x')\neq y}\right]
% \end{align*}
% The definition of this quantity is not immediate and requires the following proposition.

% \begin{prop}
% For any  Borel measurable function $h$, the adversarial risk is well defined $\risk^\varepsilon_{0/1}(h)$.
% \end{prop}
% TBC
% The existence of a minimizer for adversarial risk is a difficult question, that was partially answered in NOTYETPUBLISHED, which states, under some mild conditions, that the minimum is attained over the set of universally measurable functions.


%  define general adversarial loss.



% The question of the loss function 
\subsection{Defending against adversarial examples}

Defending against adversarial examples is still an open research questions with few answers to it. One can derive the methods in two categories: empirical defenses and provable defenses.

\paragraph{Provable defenses.} A defense is said to be provable if there is a theoretical guarantee to ensure a level of robustness. Formally, a classifier $h$ is said to \emph{certifiably robust at level $\varepsilon$} at input $x$ with label $y$ if there exist no adversarial example of level $\varepsilon$ on $h$ at the point $(x,y)$, i.e. for all $x'$ such that $d(x,x')\leq\varepsilon$, $h(x') = y$. 

\paragraph{Lipschitz property of Neural Networks} 
% TODO: define f properly
The Lipschitz constant has seen a growing interest in the last few years in the field of deep learning~\citep{scaman2018lipschitz,fazlyab2019efficient,combettes2020lipschitz,bethune2021many}.
Indeed, numerous results have shown that neural networks with a small Lipschitz constant exhibit better generalization~\citep{bartlett2017spectrally}, higher robustness to adversarial attacks~\citep{szegedy2014intriguing,farnia2018generalizable,tsuzuku2018lipschitz}, better training stability~\citep{xiao2018dynamical,trockman2021orthogonalizing}, improved Generative Adversarial Networks~\citep{arjovsky2017wasserstein}, etc.
Formally, we define the Lipschitz constant with respect to the $\ell_2$ norm of a Lipschitz continuous function $f$ as follows:
\begin{equation*}
  Lip_{2}{(f)} = \sup_{\substack{x, x' \in \XX \\ x \neq x'}} \frac{\lVert f(x) - f(x') \rVert_2}{\lVert x - x' \rVert_2} \enspace.
\end{equation*}

Intuitively, if a classifier is Lipschitz, one can bound the impact of a given input variation on the output, hence obtaining guarantees on the adversarial robustness.
We can formally characterize the robustness of a neural network with respect to its Lipschitz constant with the following proposition:
\begin{prop}[\citet{tsuzuku2018lipschitz}] \label{proposition:tsuzuku}
Let $\mathbf{f}$ be an $L$-Lipschitz continuous classifier for the $\ell_2$ norm.
Let $\varepsilon > 0$, $x \in \XX$ and $y \in \YY$ the label of $x$.
If at point $x$, the margin $\mathcal{M}_{\mathbf{f}}(x)$ satisfies:
\begin{equation*}
  \mathcal{M}_{\mathbf{f}}(x):=\max(0,f_y(x)-\max_{y'\neq y}f_{y'}(x)) > \sqrt{2} L \varepsilon
\end{equation*}
then we have for every $\tau$ such that $\lVert \tau \rVert_2 \leq \varepsilon$:
\begin{equation*}
  \argmaxB_{k}f_k(x + \tau) = y
\end{equation*}
\end{prop}
From Proposition~\ref{proposition:tsuzuku}, it is straightforward to compute a robustness certificate for a given point.
Consequently, in order to build robust neural networks the margin needs to be large and the Lipschitz constant small to get optimal guarantees on the robustness for neural networks.


Hence, many research proposed methods to build 1-Lipschitz layers in order to boost adversarial robustness. These approaches provide deterministic guarantees for adversarial robustness. One can either normalize the weight matrices by their largest singular values making the layer $1$-Lipschitz, \emph{e.g.}~\citep{yoshida2017spectral,miyato2018spectral,farnia2018generalizable,anil2019sorting} or project the weight matrices on the Stiefel manifold \citep{li2019preventing,trockman2021orthogonalizing,skew2021sahil}.
The work of \citet{li2019preventing}, \citet{trockman2021orthogonalizing} and \citet{skew2021sahil} (denoted BCOP, Cayley and SOC respectively) are examples of approaches that aims at building orthogonal weights in layers.
Indeed, their approaches consist of projecting the weights matrices onto an orthogonal space in order to preserve gradient norms and enhance adversarial robustness by guaranteeing low Lipschitz constants. 
While both works have similar objectives, their execution is different.
The BCOP layer (Block Convolution Orthogonal Parameterization) uses an iterative algorithm proposed by \citet{bjorck1971iterative} to orthogonalize the linear transform performed by a convolution.
The SOC layer (Skew Orthogonal Convolutions) uses the property that if $A$ is a skew symmetric matrix then $Q=\exp{A}$ is an orthogonal matrix. To approximate the exponential, the authors proposed to use a finite number of terms in its Taylor series expansion.
Finally, the method proposed by~\citet{trockman2021orthogonalizing} use the Cayley transform to orthogonalize the weights matrices.
Given a skew symmetric matrix $A$, the Cayley transform consists in computing the orthogonal matrix $Q = (I - A)^{-1} (I + A)$. Both methods are well adapted to convolutional layers and are able to reach high accuracy levels on CIFAR datasets. Also, several works~\cite{anil2019sorting,singla2021householder,huang2021local} proposed methods leveraging the properties of activation functions to constraints the Lipschitz of Neural Networks. These works are usually useful to help  improving the performance of linear orthogonal layers.





\paragraph{Evaluation Protocol}

Unless defense mechanisms are provable and have guarantees, evaluating Adversarial Robustness is a complicated task. Many  defenses introduced in the literature were actually proven to be ``false'' defenses. Indeed, one needs to adapt each attack to the defense. We describe the following common issues. CITE more. 

For instance, when evaluating against randomized classifiers in either white-box or black-box setting, the return output is a random variable, hence the computation of an attack against it needs to be adapted to the nature of the classifier. To do so, \cite{athalye2018obfuscated} proposed to average either the outputs or the gradient of the classifier to build an efficient attack. This procedure was called Expectation Over Transformation (EOT). 

To answer the need of adversarial examples community to evaluate accurately their models against adversarial examples, \cite{croce2020robustbench} proposed RobustBench as a unified platform for benchmarking adversarial defenses. The platform evaluates models on different attacks (AutoPGD~\citep{Croce2020ReliableEO}, FAB~\citep{Croce2020MinimallyDA}, SquareAttack~\citep{andriushchenko2019square}). The platform  does not propose to evaluate the robustness of randomized classifiers. 

\paragraph{State-of-the-art in Image Classification}

To evaluate the perfomance of an attack of a classification algorithm, one needs to train and evaluate on datasets. In image classification evaluation, three datasets are mainly used:
\begin{itemize}
    \item \textbf{MNIST~\citep{lecun1998mnist}:} A dataset of black and white low-quality images representing the $10$ digits. The training set contains $50000$ images and test set $10000$ images. These images are of dimension $28\times28\times 1$ ($784$ in total). This dataset is known to be easy ($>99\%$ can be obtained using simple classifiers). In adversarial classification, the problem is also easy to be solved. Evaluation on MNIST is not sufficient to assess the performance of a classifier or even a defense against adversarial examples.
    \item \textbf{CIFAR10 and CIFAR100~\citep{krizhevsky2009learning}:} Datasets of colored low-quality images representing the $10$ labels and $100$ labels for respectively CIFAR10 and CIFAR100. Each training set contains $50000$ images and test set $10000$ images. These images are of dimension $32\times32\times 1$ ($3072$ in total). The current state-of-the-art on CIFAR10 in standard classification is $>99\%$ of accuracy, but asks advanced methods to reach such a score. On CIFAR100, the current state-of-the-art is around $94\%$. In adversarial classification both datasets are challenging and difficult. The evolution of state-of-the-art in adversarial classification is available in RobustBench\footnote{\url{https://robustbench.github.io/}}. Benchmark in adversarial classification are often made on these datasets.
    \item \textbf{ImageNet~\citep{imagenet_cvpr09}:} ImageNet refers to a dataset containing $1.2$ million of images labeled into $1000$ classes. Images are of diverse qualities, but models often takes input of dimension $224\times224\times 3$ (dimension $150528$ in total). The current state-of-the-art on ImageNet is about $87\%$. There is no need to say that adversarial classification on ImageNet is still a very-challenging task. Further than the standard dataset, ImageNet project is still in development: the project gathers $14197122$ images and $21841$ labels on August 31th, 2021.   

\end{itemize}




\subsection{Theoretical knowledge in Adversarial classification}


\paragraph{Curse of dimensionality.} From the seminal paper~\citep{szegedy2014intriguing}, the input dimension has been considered as an argument for inevitability of adversarial attacks. To assess this intuition,~\citet{goodfelleshafahi2018adversarial} proved that for a wide range of distribution $\PP$ on the unit sphere of dimension $d$, and any classifier $h$ it is possible to find an attack on examples $x$ with high probability, exponentially depending on the dimmension $d$ on $\PP$. The arguments relies on isoperimetric inequalities and was extended to log-concave distributions on Riemannian manifolds and uniform distribution over positively curved Riemannian manifolds~\citep{pmlr-v97-dohmatob19a}. 

\citep{simon2019first} also tried to explain the exitence of adversarial examples for neural networks under the light of the high dimensionality of inputs. The authors assumed that neetworks have ReLU activations and that the distributions of weight are Gaussian. Under such hypothesis, they proved that the gradient norm with regards the input is highly dependent on the dimension of the input, then justifying again that the dimensionality of the input is a reason for existence of adversarial examples. 




\paragraph{Generalization bounds in Adversarial learning.} Similarly to the standard classification case, research have focused on computing uniform bounds for adversarial classification. These works are often inspired from generalizations of standard tools as VC-dimension~\citep{cullina2018pac} or Rademacher complexity~\citep{yin2019rademacher,khim2018adversarial,awasthi2020adversarial} is the adversarial case. They exhibit generalization bounds that are highly dependent on the dimension of the input. Indeed the Rademacher complexity for classes adapted to the adversarial case add a polynomial term in the dimension $d$ of the input. 

However, for randomized classifiers, it is difficult to adapt PAC-Bayes bounds to the adversaThere is still misunderstanding in the bias-complexity tradeoff in the adversarial case~\citep{xxx}. 

\paragraph{Adversarial Bayes Risk.} 