\section{Supervised Classification in a Nutshell}
A classification task aims at learning a function thats assigns a label to a given input. Classification is one of the two main categories of tasks along with regression. One can found classsification tasks in Computer Vision, Natural Language Processing, Speech Recognition, etc. In this thesis, most examples will be from Computer Vision and Image Recognition. 
\subsection{Notations}
In this section, we formalize the task of classification. First, we define the notions of inputs and labels:
\begin{itemize}
    \item Consider an input space $\XX$, typically images. We assume this space is endowed with an arbitrary metric $d$ possibly a perception distance or any $\ell_p$ norm. In the following of the manuscript, unless it is specified, $(\XX,d)$ will be a \textit{proper} (i.e. closed balls are compact) \textit{Polish} (i.e. completely separable) metric space. Note that for any norm $\lVert\cdot\rVert$,  $(\mathbb{R}^d,\lVert\cdot\rVert)$ is a proper Polish metric space.
    \item Each input $x\in \XX$ has to be associated with a label $y$. A label is a descriptor of the input. The set of labels is discrete and we designate it by $\YY:=\{1,\dots,K\}$. $\YY$ is endowed with the trivial metric  $d'(y,y') = \mathbf{1}_{y\neq y'}$. Note that $(\XX\times\YY,d\oplus d')$ is also a proper Polish space.
\end{itemize}

The purpose of classification is to learn a classifier $h:\XX\to\YY$. It is usual to learn a function $f:\XX\to\RR^K$ such that: $h(x) = \argmaxB_{k\in\YY}f_k(x)$. In a classification problem in machine learning, the data is assumed to be sampled from an unknown probability distribution $\PP$ over $\XX\times\YY$. We will assume from now that all the probability distributions we consider are Borel distributions. For any Polish Space $\mathcal{Z}$, we will denote $\mathcal{B}(\mathcal{Z})$ the Borel $\sigma$-algebra and the set of Borel distributions over $\mathcal{Z}$  will be denoted $\mathcal{M}_+^1(\mathcal{Z})$. We recall that on Polish space, all Borel probability distributions are Radon measures We also recall the notion of \textit{universal measurability}: a set $A\subset \mathcal{Z}$ is said to be universally measurable if it is measurable for every \textit{complete} Borel probability measure.




\subsection{Classification Task in Supervised Learning}


In standard classification, we usually aims at maximizing the accuracy of the classifier, or equivalently, at minimizing the risk associated with the $0/1$ loss defined as follows.

    
\begin{definition} Let $\PP$ be a Borel probability distribution over $\XX\times\YY$. Let $h:\XX\to\YY$ be a Borel measurable classifier. Then, the risk of $h$ associated with $0/1$ loss (or error of $h$) is defined as:
\begin{align}
   \risk_{0/1,\PP}(h):=\PP(h(x)\neq y) = \EE_{(x,y)\sim\PP}\left[\mathbf{1}_{h(x)\neq y}\right]
\end{align}
The bayes risk is defined as the optimal risk over measurable classifiers $\mathcal{F}(\XX,\YY)$:
\begin{align}
    \risk^\star_{0/1,\PP}:=\inf_{h\in\mathcal{F}(\XX,\YY)}\risk_{0/1,\PP}(h)
 \end{align}
If $f:\XX\to\RR^K$, then the risk of $f$ is defined as $\risk_{0/1,\PP}(f):=\PP(\argmaxB_{k\in\YY}f_k(x)\neq y)$
\end{definition}

Note that this quantity is well defined when $h$ or $f$ is Borel or universally measurable. The optimal classifier is called the Bayes Optimal classifier and is defined as $h(x) = \argmaxB_k\PP(y=k\mid x)$. We remark that the disintegration theorem ensures that $x\to \PP(y=k\mid x)$ is indeed Borel measurable. 

In practice, the access to the Bayes Optimal classifier is not possible because it requires full knowledge of the probability distribution $\PP$ which is not the case in general. Instead, in the supervised learning setting, the learner has access to data points $\{(x_1,y_1),\dots,(x_\numsamples,y_\numsamples)\}$, that constitutes the \emph{training set}. Knowing the Bayes Optimal classifier on training points is not sufficient to generalize on points out of the training set. Hence one need to reduce the search space of measurable functions to a much smaller one, denoted $\mathcal{H}$ in the sequel. The $0/1$ loss is not convex neither continuous, and minimizing directly the 0/1 loss risk  on $\mathcal{H}$ is  NP-hard CITE. We usually minimize a well-chosen surrogate loss function $\loss$. A \textit{loss function} $\loss:\mathbb{R}^K\times\YY\to\mathbb{R}$ is a non negative Borel measurable function. An example of such a loss is the cross entropy loss defined as $\loss(\mathbf{f}(x),y)=-\sum_{i=1}^K \mathbf{1}_{y=i}\log f_i(x)$
where $f_i(x)$ is the probability learnt by the model that input $x$ belongs to the class $i$. Hence the objective is to minimize the empirical risk associated over $\mathcal{H}$ with the loss $\loss$  defined as:

\begin{align*}
\riskemp_{\loss}(\mathbf{f}):= \frac{1}{\numsamples}\sum_{i=1}^\numsamples \loss(f(x_i),y_i).
\end{align*}

Standard examples of losses are the cross-entropy loss or the margin losses. In the coming sections~\ref{xxx} and~\ref{xxx}, we recall the main results about surrogate losses for the $0/1$ loss and generalization properties of the classifiers.



\paragraph{Usual Datasets in Image Classification}
%Images are embeddings in pixels laying in $[0,255]$ and then normalized to $[0,1]$. These images can be black and white, hence encoded on only one channel, or colorful and then encoded on three channels, often, Red, Green, Blue (RGB). The images are of diverse qualities, the number of pixels quantifies this quality.  
In this PhD thesis, although the results are general and applicable to every type of data, we mainly focus on image classification tasks. Three datasets are mainly used in image classification:
\begin{itemize}
    \item \textbf{MNIST~\citep{lecun1998mnist}:} A dataset of black and white low-quality images representing the $10$ digits. The training set contains $50000$ images and test set $10000$ images. These images are of dimension $28\times28\times 1$ ($784$ in total). This dataset is known to be easy ($>99\%$ can be obtained using simple classifiers). 
    \item \textbf{CIFAR10 and CIFAR100~\citep{krizhevsky2009learning}:} Datasets of colored low-quality images representing the $10$ labels and $100$ labels for respectively CIFAR10 and CIFAR100. Each training set contains $50000$ images and test set $10000$ images. These images are of dimension $32\times32\times 3$ ($3072$ in total). The current state-of-the-art on CIFAR10 in standard classification is $>99\%$ of accuracy, but asks advanced methods to reach such a score. On CIFAR100, the current state-of-the-art is around $94\%$. 
    \item \textbf{ImageNet~\citep{imagenet_cvpr09}:} ImageNet refers to a dataset containing $1.2$ million of images labeled into $1000$ classes. Images are of diverse qualities, but models often takes input of dimension $224\times224\times 3$ (dimension $150528$ in total). The current state-of-the-art on ImageNet is about $87\%$. Further than the standard dataset, ImageNet project is still in development: the project gathers $14197122$ images and $21841$ labels on August 31th, 2021.   


\end{itemize}

\subsection{Surrogate losses, consistency and calibration} 

\paragraph{Binary Classification.} In this section, we recall the main results about surrogate losses in binary classification. We assume that $\YY = \{-1,+1\}$. In this case, a classifier is a function $f:\XX\to\RR$ such that an input $x$ is classified as $1$ if $f(x)>0$ and as $-1$ if $f(x)\geq 0$. Then the $0/1$ loss is defined as $\mathbf{1}_{ysign(f(x))\leq0}$. As mentioned earlier, optimizing the risk associated $0/1$ loss is a difficult task, generally an NP-hard problem~\cite{xxx}. We need to properly introduce notions of surrogate losses. 

A margin loss is a measurable function $\phi:\RR\to\RR_+$ defined as $\phi(yf(x))$. The risk associated with a margin loss $\phi$ is then $\risk_{\phi,\PP}(f):=\mathbb{E}_\PP\left[\phi(yf(x))\right]$. A loss $\phi$ is said to be \emph{classification-consistent} if every minimizing sequence for the $\phi$-risk is also a minimizing sequence for the $0/1$-risk. In other words, for a given $\PP\in\mathcal{M}_1^+(\XX\times\YY)$, $\phi$ is classification-consistent for $\PP$ if for all sequences $(f_n)_n$ of measurable functions:
\begin{align}
    \mathcal{R}_{\phi,\PP}(f_n)\to \mathcal{R}_{\phi,\PP}^\star\implies\mathcal{R}_{\PP}(f_n)\to \mathcal{R}_{\PP}^\star
\end{align}

 While this notion seems complicated to study,~\cite{bartlett2006convexity,steinwart2007compare} have focused on a relaxation named \emph{calibration}. A loss is said to be classification calibrated if for every $\varepsilon>0$, there exists $\delta>0$ such that for every $f$ measurable and $\eta\in[0,1]$:
\begin{align*}
    ccc
\end{align*}
We remark the notion of calibration is basically a pointwise notion of consistency.~\cite{bartlett2006convexity,steinwart2007compare} proved the equivalence of the two notions in the case of standard-binary classification. In particular they show that a wide range of convex margin losses are actually classification-consistent: if $\phi$ is convex and differnetiable at $0$, then $\phi$ is calibrated if and only if $\phi'(0)<0$.





\paragraph{Beyond binary-classification.}

% We say that a loss $L_2$ is said to be consistent with respect to a loss $L_1$ if consistency holds for every distribution $\PP$. 

% \begin{definition}[Loss function] A loss function is a function $\loss:\XX\times\YY\times \mathcal{F}(\XX)\to \mathbb{R}$ such that $\loss(\cdot,\cdot,f)$ is a Borel measurable for all $f\in\mathcal{F}(\XX)$. 
% \end{definition}


% Similarly to risk associated with the $0/1$ loss, 
% \begin{definition}[$\loss$-risk of a classifier]
% For a given measurable loss $\loss$, and a Borel probability distribution $\PP$ over $\XX\times\YY$ we define the risk of classifier $f$ associated with the loss $\loss$ and a distribution $\PP$ as:
% \begin{align*}
%      \mathcal{R}_{\loss,\PP}(f) := \mathbb{E}_{(x,y)\sim\PP}\left[\loss(x,y,f)\right].
% \end{align*}
% We also define the optimal risk associated with the loss $\loss$:
% \begin{align*}
%          \mathcal{R}_{\loss,\PP}^\star := \inf_{f\in\mathcal{F}(\XX)}\mathcal{R}_{\loss,\PP}(f)
% \end{align*}
% \end{definition}


% The risk of a classifier is then defined as the average loss over the distribution $\PP$. The loss $L$ is difficult to optimize in practice: the loss might, for instance, not convex  not differentiable or continuous. It is then often preferred to optimize a surrogate loss function instead. In the literature~\citep{zhang2004statistical,bartlett2006convexity,steinwart2007compare}, the notion of surrogate losses has been studied as a consistency problem: a surrogate loss is said to be consistent if any minimizing sequence of classifiers for the  risk associated with the surrogate loss is also one for the risk associated with $L$. Formally, the notion of consistency as  follows.

% \begin{definition}[Consistency]
% Let $L_1$ and $L_2$ be two loss functions. For a given $\PP\in\mathcal{M}_1^+(\XX\times\YY)$, $L_2$ is said to be consistent for $\PP$ with respect to $L_1$ if for all sequences $(f_n)_n \in \mathcal{F}(\XX)^\mathbb{N}$ :
% \begin{align}
%     \mathcal{R}_{L_2,\PP}(f_n)\to \mathcal{R}_{L_2,\PP}^\star\implies\mathcal{R}_{L_1,\PP}(f_n)\to \mathcal{R}_{L_1,\PP}^\star
% \end{align}
% We say that a loss $L_2$ is said to be consistent with respect to a loss $L_1$ if consistency holds for every distribution $\PP$.
% \end{definition}

\subsection{Empirical Risk Minimization and Generalization}

As mentioned earlier, the learner has access to training points  $\{(x_1,y_1),\dots,(x_\numsamples,y_\numsamples)\}$ and not to the whole distribution. We aim at learning the classifier on a class of function $\mathcal{H}$. The classifier $\hat{f}_\numsamples $ is then chosen to minimize the empirical risk given a loss $\loss$:
\begin{align*}
    \hat{f}_\numsamples = \argminB_{f\in\mathcal{H}}\riskemp_{\loss}(\mathbf{f})= \argminB_{f\in\mathcal{H}}\frac{1}{\numsamples}\sum_{i=1}^\numsamples \loss(f(x_i),y_i).
\end{align*}

Since the learning procedure takes into account 


\paragraph*{Risk Decomposition and bias-complexity tradeoff.} The excess risk of a classifier is defined as the difference between the risk and the optimal risk: $\risk_\loss(f_n) - \risk_\loss^\star$. The excess risk can be decomposed as follows:
\begin{align*}
    \risk_\loss( \hat{f}_\numsamples) - \risk_\loss^\star = \left(\risk_\loss( \hat{f}_\numsamples) - \risk_{\loss,\mathcal{H}}^\star\right)+ \left(\risk_{\loss,\mathcal{H}}^\star - \risk_\loss^\star\right)
\end{align*}

with $\risk_{\loss,\mathcal{H}}^\star = \inf_{f\in\mathcal{H}}\risk_\loss(f)$. The two terms in the previous decomposition corresponds respectively to:
\begin{itemize}
    \item \textbf{The estimation risk}:  the empirical risk $\risk(\hat{f}_\numsamples)$ (i.e., training error) is only an estimate of the optimal risk, and so $\hat{f}_\numsamples$   is only an estimate of the predictor minimizing the true risk. The estimation risk depends on the training set size $\numsamples$ and on the size, or complexity, of  $\mathcal{H}$.  The more samples we have the smaller will be the estimation risk and more complex $\mathcal{H}$ is the larger the estimation error will be.
    \item \textbf{The approximation risk}: 
\end{itemize}

This decomposition induces a tradeoff on the complexity of $\mathcal{H}$ named \emph{bias-complexity tradeoff} or \emph{bias-variance tradeoff}. On one hand, if $\mathcal{H}$ is not enough rich, then the estimation risk would be small but the approximation error can be large, it is called \emph{underfitting}. On the other hand, if $\mathcal{H}$ is is too rich, then the approximation risk would be small but the estimation error large, it is called \emph{overfitting}. To overcome these issues in practice, it is usual to add a regularization parameter to the empirical risk depending on the class $\mathcal{H}$:
\begin{align*}
    \hat{f}_\numsamples = \argminB_{f\in\mathcal{H}}\riskemp_{\loss}(\mathbf{f}) +\lambda\times\Omega_\mathcal{H}(f)= \argminB_{f\in\mathcal{H}}\frac{1}{\numsamples}\sum_{i=1}^\numsamples \loss(f(x_i),y_i)+\lambda\times\Omega_\mathcal{H}(f).
\end{align*}

For instance when $\mathcal{H}$ is a Reproducing Kernel Hilbert Space, the regularization parameter is the square norm of $f$: $\lVert f\rVert_\mathcal{H}$. The risk analysis for RKHS in the regularized least squares regression case have been widely studied in~\cite{xxxx} 


\paragraph{Uniform Convergence.} Since $\hat{f}_\numsamples$ is dependent on the training samples, it is usually difficult to estimate $\risk(\hat{f}_\numsamples)$ from training samples. A natural thing to do is to upperbound this quantity using:
\begin{align*}
    \lvert\riskemp(\hat{f}_\numsamples) - \risk(\hat{f}_\numsamples)\rvert\leq\sup_{f\in\mathcal{H}}    \lvert\riskemp(f) - \risk(f)\rvert
\end{align*}

The convergence of the right-end term is referred as uniform convergence or PAC-learning. It can be bounded either with high probability or in expectation (i.e. $L^1$ convergence). We remark the speed of convergence depends on the complexity of $\mathcal{H}$: more complex $\mathcal{H}$ is, the slower the convergence will be, hence exhibiting again a tradeoff on the expressivity of $\mathcal{H}$. There have been a  lot of research that proposed tools to study this convergence. Now, we recall two fundamental tools, namely the VC-dimension and the Rademacher complexity.

TODO: Rademacher + VC


The Rademacher complexity was introduced by