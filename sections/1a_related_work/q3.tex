\section{Q3}

We recall a classifier $h$ is \emph{certifiably robust at level $\varepsilon$} at input $x$ with label $y$ if there exist a property depending on $h$, $x$, $y$ and $\varepsilon$ that implies that for all $x'$ such that $d(x,x')\leq\varepsilon$, $h(x') = y$.




\subsection{Lipschitz Property of Neural Networks}


The Lipschitz constant has seen a growing interest in the last few years in the field of deep learning~\citep{scaman2018lipschitz,fazlyab2019efficient,combettes2020lipschitz,bethune2021many}.
Indeed, numerous results have shown that neural networks with a small Lipschitz constant exhibit better generalization~\citep{bartlett2017spectrally}, higher robustness to adversarial attacks~\citep{szegedy2014intriguing,farnia2018generalizable,tsuzuku2018lipschitz}, better training stability~\citep{xiao2018dynamical,trockman2021orthogonalizing}, improved Generative Adversarial Networks~\citep{arjovsky2017wasserstein}, etc.
Formally, we define the Lipschitz constant with respect to the $\ell_2$ norm of a Lipschitz continuous function $f$ as follows:
\begin{equation*}
  Lip_{2}{(f)} = \sup_{\substack{x, x' \in \XX \\ x \neq x'}} \frac{\lVert f(x) - f(x') \rVert_2}{\lVert x - x' \rVert_2} \enspace.
\end{equation*}

Intuitively, if a classifier is Lipschitz, one can bound the impact of a given input variation on the output, hence obtaining guarantees on the adversarial robustness.
We can formally characterize the robustness of a neural network with respect to its Lipschitz constant with the following proposition:
\begin{prop}[\citet{tsuzuku2018lipschitz}] \label{proposition:tsuzuku}
Let $f:\XX\to\RR^K$ be an $L$-Lipschitz continuous classifier for the $\ell_2$ norm.
Let $\varepsilon > 0$, $x \in \XX$ and $y \in \YY$ the label of $x$.
If at point $x$, the margin $\mathcal{M}_{\mathbf{f}}(x)$ satisfies:
\begin{equation*}
  \mathcal{M}_{\mathbf{f}}(x):=\max(0,f_y(x)-\max_{y'\neq y}f_{y'}(x)) > \sqrt{2} L \varepsilon
\end{equation*}
then we have for every $\tau$ such that $\lVert \tau \rVert_2 \leq \varepsilon$:
\begin{equation*}
  \argmaxB_{k}f_k(x + \tau) = y
\end{equation*}
\end{prop}
From Proposition~\ref{proposition:tsuzuku}, it is straightforward to compute a robustness certificate for a given point.
Consequently, in order to build robust neural networks the margin needs to be large and the Lipschitz constant small to get optimal guarantees on the robustness for neural networks.

\paragraph{Lipschitz Constant of Neural Networks.}  A neural network is a function $f$ defined succession of linear and non-linear activation functions $\sigma$:
\begin{align*}
  f(x) = \left(A_L\sigma\left(A_{L-1}\dots \sigma\left(A_1x+b_1\right)\dots\right)+b_L\right)
\end{align*}
Assuming that $\sigma$ is $1$-Lipschitz, we have:
\begin{align*}
  \lVert f(x)-f(y)\rVert_2\leq \lVert A_1\rVert_2\dots \lVert A_L\rVert_2\lVert x-y\rVert_2
\end{align*}
with $\lVert A\rVert_2$ is the spectral norm of $A$ defined as
\begin{align*}
  \lVert A\rVert_2 = \max_{x\neq 0} \frac{\lVert Ax\rVert_2}{\lVert x\rVert_2} = \lambda_{max}(A^TA)\quad.
\end{align*}
where $\lambda_{max}(A^TA)$ denotes the greatest eigen value of $A^TA$. Note that $\lVert A\rVert_2$ is also the greatest singular value of $A$.  
Then the Lipschitz constant of $f$ is upperbounded by $\lVert A_1\rVert_2\dots \lVert A_L\rVert_2$. Hence to control the Lipschitz constant of a neural network, it is usual to control the spectral norm of each layer. It could be done either in penalizing this upperbound or imposing a spectral norm equals smaller than $1$ for each layer. 

\subsection{Learning Lipschitz layers}

Many research proposed methods to build 1-Lipschitz layers in order to boost adversarial robustness. These approaches provide deterministic guarantees for adversarial robustness. One can either normalize the weight matrices by their largest singular values making the layer $1$-Lipschitz, \emph{e.g.}~\citep{yoshida2017spectral,miyato2018spectral,farnia2018generalizable,anil2019sorting} or project the weight matrices on the Stiefel manifold \citep{li2019preventing,trockman2021orthogonalizing,skew2021sahil}.

The first natural idea to learn $1$-Lipschitz layers is to normalize the matrices in the forward pass of a Neural Networks : $A_i\leftarrow \frac{A_i}{\lVert A_i\rVert_2}$. This natural idea was exploited by~\citet{miyato2018spectral}. A key difficulty is the computation of the spectral norm $\lVert A_i\rVert_2$. The authors proposed to use the power iteration method to compute the spectral norm (see Algorithm~\ref{xxx}). The number of iterations might be prohibitive, hence the authors proposed to use only one step in the training phase to make it faster. This method effectively approximated well the sprectral norm of the last layer. However, this method present some disadvantages. The spectral normalization has for effect crushing all smaller singular values. A consequence is the gradient vanishing that is very present in this structure. 

Also, several works~\cite{anil2019sorting,singla2021householder,huang2021local} proposed methods leveraging the properties of activation functions to constraints the Lipschitz of Neural Networks. These works are usually useful to help  improving the performance of linear orthogonal layers.



\paragraph{Learning Orthogonal layers} A workaround for the limitations of previously presented methods is to build norm preserving linear layers, i.e. orthogonal layers. We recall a matrix $\Omega\in\RR^{d\times d}$ is said to be orthogonal if for every $x\in\RR^d$, $\lVert\Omega x\rVert_2 = \lVert x\rVert_2$. Indeed such layers exactly preserve the norm, hence avoid crushing all singular values and gradient vanishing issues. Recently, there have been a trend in aiming at learning Orthogonal Layers in neural networks.  The following approaches consist of projecting the weights matrices onto an orthogonal space in order to preserve gradient norms and enhance adversarial robustness by guaranteeing low Lipschitz constants. While both works have similar objectives, their execution is different .It is a difficult question to conciliate the convolution structure with orthogonality of linear layers. The presented works of \citet{li2019preventing}, \citet{trockman2021orthogonalizing} and \citet{skew2021sahil} (denoted BCOP, Cayley and SOC respectively) present the advantage of being ``compatible''  with convolutional structure in layers. 

The BCOP layer (Block Convolution Orthogonal Parameterization) uses an iterative algorithm proposed by \citet{bjorck1971iterative} to orthogonalize a linear transformation. The BCOP layer relies on the following algorithm to orthonormalize a linear operator $M$:
\begin{align*}
    xxxx
\end{align*}
To build a ``convolutional layer'' from the BCOP the matrix $M$ can be structured as a convolutional operator is a standard Deep Learning framework as Tensorflow~\citep{abadi2016deep} or PyTorch~\citep{paszke2019pytorch}. TODO DESCRIBE

Two other alternatives, the SOC layer (Skew Orthogonal Convolution) and the Cayley layer, used two different parametrization of the Special Orthogonal Group $SO_n(\RR)$ using skew-symmetric matrices. Indeed, in Riemmanian geometry, the space skew-symmetric matrices is isomorphic to the tangent space of $SO_n(\RR)$ at any point. 

SOC layers uses the expontial mapping of a skew symmetric matrix defined using the following Taylor expansion:
\begin{align*}
  \exp{A}:=\sum_{k=0}^{\infty}\frac{A^k}{k!}
\end{align*}
which defines an orthogonal matrix, indeed $(\exp{A})^T\exp{A} = \exp(A^T)\exp(A) = \exp(-A)\exp(A) = \exp(A-A)=I$ . More precisely, the application $A\mapsto\exp{A}$ defines a surjective mapping of $SO_n(\RR)$ from the space of skew-symmetric matrices. To approximate the exponential of a matrix, the authors proposed to use a finite number of terms in its Taylor series expansion.  To be adapted to convolutions, a skew-symmetric linear transformation $A = M-M^T$ can be  computed in a Deep Learning Framework using the  convolution and convolution-transpose operators.



The Cayley method proposed by~\citet{trockman2021orthogonalizing} use the Cayley transform to orthogonalize the weights matrices. Given a skew symmetric matrix $A$, the Cayley transform consists in computing the orthogonal matrix:
\begin{align*}
   \text{Cayley}(A)= (I - A)^{-1} (I + A) \quad.
\end{align*}
Like exponential mapping, the Cayley Tranform defines  a surjective mapping of $SO_n(\RR)$ from the space of skew-symmetric matrices. TODO: adaptabiltilty to skew





\subsection{Residual Networks}

During the training phase in neural networks, it may occur some issues as gradient vanishing or gradient expolosion~\citep{hochreiter2001gradient}. These issues limited the emergence of scalable and very deep neural networks until~\cite{he2016deep} proposed the Residual Network (ResNet) architecture defined as follows.
\begin{align*}
  \left\{
    \begin{array}{ll}
    x_0 &= x\in\XX\\
    x_{t+1} &= x_t+F_{{t}}(x_{t}) \  \text{for } \ t\in\{0, \dots,T\}
  \end{array}
  \right.
\end{align*}
where $F_{{t}}(x_{t})$ is typically a two layer neural networks. The ResNet uses residual connection that have the effect of limiting gradient vanishing issues. Combined with batch normalization, the issue of gradient explosion can also be mitigated, hence opening the possibility to very deep and stable architecture. 

To theoretically analyse the ResNet architecture, several works~\citep{haber2017stable,e17Proposal,lu18beyond,chen2018neural} proposed a ``continuous time'' interpretation inspired by dynamical systems that can be defined as follows.

\begin{definition}\label{def:flow}
Let $(F_{t})_{t\in[0,T]}$ be a family of functions on $\RR^d$, we define the continuous time Residual Networks flow associated with $F_t$ as:
\begin{align*}
  \left\{
    \begin{array}{ll}
    x_0 &= x\in\mathcal{X}\\
    \frac{dx_{t}}{dt} &= F_{{t}}(x_{t}) \  \text{for } \ t\in[0, T]
  \end{array}
  \right.
\end{align*}
\end{definition}

This continuous time interpretation helps as it allows us to consider the stability of the forward propagation through the stability of the associated dynamical system.
A dynamical system is said to be \emph{stable} if two trajectories starting from an input and another one remain sufficiently close to each other all along the propagation. This stability property takes all its sense in the context of adversarial classification.

It was argued by~\citet{haber2017stable} that when $F_{t}$ does not depend on $t$ or vary slowly with time\footnote{This blurry definition of "vary slowly" makes the property difficult to apply.}, the stability can be characterized by the eigenvalues of the Jacobian matrix $\nabla_x F_{t}(x_t)$: 
the dynamical system is stable if the real part of the eigenvalues of the Jacobian stay negative throughout the propagation.
This property however only relies on intuition and this condition might be difficult to  verify in practice.
In the following, in order to derive stability properties, we study gradient flows and convex potentials, which are sub-classes of Residual networks.

Other works~\citep{huang2020adversarial,li2020implicit} also proposed to enhance adversarial robustness using dynamical systems interpretations of Residual Networks. Both works argues that using particular discretization scheme would make gradient attacks more difficult to compute due to numerical stability. These works did not provide any provable guarantees for such approaches.



