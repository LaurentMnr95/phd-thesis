In this chapter, we present all relevant related works to answer the questions Q1, Q2, and Q3. 

\section{Q1}

% Optimal Transport have gained interest in Machine Learning applications during the past years. Indeed, Optimal Transport has the ability to model many problems as Generative Adversarial Networks~\citep{arjovsky2017wasserstein}, or in Robust Learning~\citep{xxx}. The computation methods for optimal transport problems have also been considerably improved recently.  We recall the main concepts from the Optimal Transport Theory. Originally introduced by Monge, this Optimal Transport was a problem where the aim was to move some quantity $x$ to some places $y$ while minimizing the total cost of transport. Formally, the problem was c



% \begin{definition}[Couplings between distributions] 
%     Let $\ZZ$ be a Polish space. Let $\PP$ and $\QQ$ be two Borel probability distributions over $\ZZ$. The set of coupling distributions between $\PP$ and $\QQ$ is defined as:
%     \begin{align*}
%         \Gamma_{\PP,\QQ}:=\left\{\gamma\in\mathcal{M}^1_+(\ZZ^2)\mid~ \Pi_{1,\sharp}\gamma = \PP,~\Pi_{2,\sharp}\gamma = \QQ\right\}
%     \end{align*}
% where $\Pi_{i,\sharp}$ represents the push-forward of the projection on the $i$-th component.
% \end{definition}
% \begin{definition}[Optimal Transport]
% Let $\ZZ$ be a Polish space. Let $c:\ZZ\to\bar{\RR}_+$ be a lower semi-continuous non-negative function. Let $\PP$ and $\QQ$ be two  Borel probability distributions over $\ZZ$. The Optimal Transport problem or Wasserstein problem between $\PP$ and $\QQ$ associated with cost function $c$ is defined as:
% \begin{align*}
%     W_c(\PP,\QQ):=\inf_{\gamma\in\Gamma_{\PP,\QQ}}\int c(x,y) d\gamma(x,y) = \inf_{\gamma\in\Gamma_{\PP,\QQ}}\mathbb{E}_{(x,y)\sim\gamma}\left[c(x,y) \right]
% \end{align*} 
% \end{definition}
% The infimum is attained.
% When $\XX$ is endowed with ground metric $d$, one can endow the space of probability distributions with bounded $p$-moments with a metric named the Wasserstein-$p$ metric defined as:
% \begin{align*}
%     D_p(\PP,\QQ):=\inf_{\gamma\in\Gamma_{\PP,\QQ}}\mathbb{E}_{(x,y)\sim\gamma}\left[d^p(x,y) \right]^{1/p}
% \end{align*}
% With this metric, the space of probability distributions with bounded $p$-moments metrizes the weak topology of measures. When $p=\infty$, the $D_\infty$ be be defined in the limit as:
% \begin{align*}
%     D_\infty(\PP,\QQ):=\inf_{\gamma\in\Gamma_{\PP,\QQ}}\gamma-\esssup_{(x,y)} d(x,y) 
% \end{align*}
% \paragraph{Kantorovich Duality.} A fundammental theorem in Optimal Transportation is the Kantorovich duality theorem as follows:

% \begin{thm}[Kantorovich duality]
%     Let $\ZZ$ be a Polish space. Let $c:\ZZ\to\bar{\RR}_+$ be a lower semi-continuous non-negative function. Let $\PP$ and $\QQ$ be two Borel probability distributions over $\ZZ$. Then   the following strong duality theorem holds:
% \begin{align*}
%     W_c(\PP,\QQ)=\sup_{f,g\in C(\ZZ),~f\oplus g\leq c}   \int fd\PP+\int fd\QQ
% \end{align*}
% where for all $x,y\in\ZZ$, $f\oplus g(x,y):=f(x)+g(y)$. 
% \end{thm}
% One can find a proof of this result in~\citep{villani2003topics}. The main arguments are that the dual of continuous functions on  a compact space is the space of Radon measures, and the Rockafellar duality theorem. 

\subsection{Adversarial Risk Minimization and Optimal Transport} Optimal Transport is a key element when studying Adversarial Classification problems. In particular, it was shown byLet $\PP$ be a distribution er the input-label space $\XX\times\YY$. We recall that the problem of adversarial risk minimization is defined as
\begin{align*}
    \risk_{\varepsilon,\PP}^\star = \inf_{h} \PP_{(x,y)}\left[\exists x'\in B_\varepsilon(x),~h(x')\neq y\right]
\end{align*}
A recent line of work draw important links between   $\risk_{\varepsilon,\PP}^\star$ and Optimal Transport problems in the case of binary classification ($\YY=\{-1,+1\}$) the space $\XX$ satisfy a midpoint property, i.e. for all $x_1,x_2\in\XX$ there exist $x\in\XX$ such that $d(x,x_1) = d(x,x_2) =\frac{d(x_1,x_2)}{2}$. It was shown that in this case:
\begin{align*}
\risk_{\varepsilon,\PP}^\star = \frac12-\frac12 W_{c_\varepsilon}(\PP,\PP^S)
\end{align*}
where $\PP^S := T^S_\sharp \PP$ with $T^S(x,y) = (x,-y)$ and
\begin{align*}
    c_\varepsilon\left((x,y),(x',y')\right) = \mathbf{1}_{d(x,x')>2\varepsilon,y\neq y'}
\end{align*}

Note that $T^S$ only switches the label of pair $(x,y)$. When $\varepsilon=0$, $W_{c_\varepsilon}(\PP,\PP^S)$ equals the total variation distance between $\PP$ and $\PP^S$, which was a result proved in~\citep{xxx}.
While this property does not have practical properties yet, there is a hope that this relation might help at building more robust classifiers to adversarial examples. 

\subsection{Distributionally Robust Optimization}

In this section, we give an introduction to distributionally robust optimization problems. We see these problems are related to adversarial attacks problems. We draw links in this introduction.  Let $\ZZ$ and $\Theta$ be Polish spaces. Let $\PP$ be a Borel probability distribution over $\ZZ$. Let $f:\Theta\times\ZZ\to\RR$ be an upper semi continuous function in its second variable. Let us consider the following problem:
\begin{align}
    \label{eq:min-objective}
    \min_{\theta\in\Theta} \mathbb{E}_{z\sim\PP}\left[f(z)\right] = \min_{\theta\in\Theta} \int f(\theta,z)d\PP(z)
\end{align}
This problem can typically be a risk minimization problem in Machine Learning when $\PP$ is a distribution over input-label pairs and $\Theta$ is a parameter space for the classifier. A distributionally robust optimization (DRO) problem is a problem similar to Equation~\eqref{eq:min-objective}, but the learner aims at being robust to a change in the distribution $\PP$. Typically if $D$ is an uncertainty metric for distribubtions. Formally, the DRO problem is casted as follows:
\begin{align*}
    \min_{\theta\in\Theta}\sup_{\QQ\in\mathcal{M}^1_+(\ZZ)\mid~D(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right]
\end{align*}
For instance, $D$ be a Kullback-Leibler etc CITE

In the case of Wasserstein uncertainty sets, let $c:\ZZ\to\bar{\RR}_+$ be a lower semi-continuous non-negative function. Then a  Wasserstein distributionally robust optimization (DRO) problem is defined as follows:
\begin{align*}
    \min_{\theta\in\Theta}\sup_{\QQ\in\mathcal{M}^1_+(\ZZ)\mid~W_c(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right]
\end{align*}

Then we can define the Wasserstein balls as 
\begin{align*}
    \mathcal{B}_{c}(\PP,\varepsilon) := \left\{\QQ\in \mathcal{M}^+_1(\mathcal{Z})\mid W_c(\PP,\QQ)\leq \varepsilon\right\}
\end{align*}
This problem induces an attack on the distribution $\PP$. Informally, one can interpret a Wassertein ball as an attacker moving each point $x$ of the distribution $\PP$ to a distribution $\QQ_x$ so that the average ``distance'' $\mathbb{E}_{x\sim\PP}\left[\mathbb{E}_{y\sim\QQ_x}\left[c(x,y)\right]\right]$ at most equal to $\varepsilon$. With this interpretation, we can start linking the Wasserstein DRO problem to the adversarial learning problem. Indeed in the adversarial attack problem, the attacker is authorized to move each point to another at distance at most $\varepsilon$, i.e. he is authorized a mapping $T$ such that $d(x,T(x))\leq \varepsilon$ for every $x$ almost surely. 
\paragraph{Properties of Wasserstein balls.} The Wasserstein balls inherits from nice properties. Since $\QQ\mapsto  W_c(\PP,\QQ)$ is convex, they are convex sets. Moreover the function $\QQ\mapsto  W_c(\PP,\QQ)$ is lower semi-continuous for the narrow topology of measures, then the set $\mathcal{B}_{c}(\PP,\eta) $ is closed for the narrow topology too. Concerning the compactness of this set, if $\ZZ$ is compact then the set $\mathcal{B}_{c}(\PP,\eta) $ is also compact as a closed subset of the compact set $\mathcal{M}^+_1(\mathcal{Z})$.~\cite{yue2020linear} proved the compactness for $l^p$ distances. In general, compactness is a case by case question. 


\paragraph{Duality results} The problem of computing DRO solutions is difficult become it concerns optimization over distribution. A strong duality leading to a relaxation of the problem was proved by~\cite{blanchet2019quantifying}. We state this theorem as follows.


\begin{thm}[Wasserstein DRO duality]
    Let $\PP$ be a Borel probability distribution over $\ZZ$. Let $f:\ZZ\to\RR$ be an upper semi continuous function. Let $c:\ZZ\to\RR_+$ be a lower semi-continuous non-negative function. 
    \begin{align*}
        \sup_{\QQ\in\mathcal{M}^1_+(\ZZ)\mid~W_c(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right] = \inf_{\lambda\geq 0}\mathbb{E}_{z\sim\PP}\left[\sup_{z'\in\ZZ}f(z')-\lambda c(z,z')\right] +\lambda\varepsilon
    \end{align*}
  
\end{thm}

This theorem was proved by~\citep{blanchet2019quantifying} using similar arguments to Kantorovich duality. The link with the adversarial attack problem is made clearer with this theorem. Indeed $\mathbb{E}_{z\sim\PP}\left[\sup_{z'\in\ZZ}f(z')-\lambda c(z,z')\right]$ is closed to the adversarial attacks problem. We will make a direct link in the Chapter~\ref{chap:game}. %We next show that the adversarial attack problem is a Wass


\paragraph{Adversarial classification as a Wasserstein-$\infty$ DRO problem.} The adversrial attack problem was studied under the light of DRO from a statistical point of view~\citep{xxx}, or to prove that adversarial classification is exactly a Wasserstein-$\infty$ problem with a well-suited cost function~\citep{xxx}. The previous result from~\citep{blanchet2019quantifying} does not directly apply to Wasserstein-$\infty$ distances but can be adapted. The Wasserstein-$\infty$ DRO problem can be understood as follows: each point $x$ of the distribution $\PP$ can be moved to a distribution $\QQ_x$ so that the worst-case ``distance'' $c(x,y)$ is smaller that $\varepsilon$. In general, one can prove the following result that proves that the adversarial classification problem is actually a Wasserstein-$\infty$ DRO problem.

\begin{thm}[Duality for Wasserstein-$\infty$ DRO]
Let $\ZZ$ be a Polish space. Let $\PP$ be a probability distribubtion over $\ZZ$. Let $c$ be a non-negative lower-semicontinuous function over $\ZZ^2$ and $f:\ZZ\to \RR$ be a Borel measurable function. Then the following strong duality holds
    \begin{align*}
        \sup_{\QQ|~W_{\infty,c}(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right] = \mathbb{E}_{z\sim\PP}\left[\sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')\right]
    \end{align*}  
\end{thm}
This result can be found in special case in~\citep{xxx}. For sake of completeness, we provide a proof of the result. 
\begin{proof}
     Let us define:
     \begin{align*}
        \Tilde{f}:(z,z')\in\ZZ^2\mapsto f(z')-\infty*\mathbf{1}_{c(z,z')>\varepsilon}\quad. 
     \end{align*}
     $\Tilde{f}$ is Borel-measurable, hence upper semi-analytic~\citep[Chapter 7]{bertsekas2004stochastic}. We then deduce that 
     \begin{align*}
       z\in\ZZ\mapsto \sup_{z'\in\ZZ} \Tilde{f}(z,z') = \sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')
     \end{align*}
     is universally measurable, hence justifying the definition of the left-end term in the Theorem. 

     Now let $\QQ$ such that $W_{\infty,c}(\PP,\QQ)\leq \varepsilon$. There exists $\gamma\in\Gamma_{\PP,\QQ}$ such that $c(z,z')\leq \varepsilon$ $\gamma$-almost surely. Then we deduce

     \begin{align*}
        \mathbb{E}_{z'\sim\QQ}\left[f(z')\right] &= \mathbb{E}_{(z,z')\sim\gamma}\left[f(z')\right]\leq\mathbb{E}_{(z,z')\sim\gamma}\left[\sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')\right] \\
       & \leq\mathbb{E}_{z\sim \PP}\left[\sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')\right]
      \end{align*}
      Hence we deduce that 
      \begin{align*}
        \sup_{\QQ|~W_{\infty,c}(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right] \leq\mathbb{E}_{z\sim\PP}\left[\sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')\right]
      \end{align*}
     
    Thanks to~\citet[Proposition 7.50]{bertsekas2004stochastic}, for any $\delta>0$, there exists a universally measurable mapping $T:\ZZ\to\ZZ$ such that $\tilde{f}(z,T(z))\geq \sup_{z'\in\ZZ} \Tilde{f}(z,z') -\delta $ for every $z\in\ZZ$. Defining $\QQ =T_\sharp\PP$, we get that $W_{\infty,c}(\PP,\QQ)\leq \varepsilon$ and that:
    \begin{align*}
        \sup_{\QQ|~W_{\infty,c}(\PP,\QQ)\leq \varepsilon}\mathbb{E}_{z\sim\QQ}\left[f(z)\right] \geq\mathbb{E}_{z\sim\PP}\left[\sup_{z'\in \ZZ|~c(z,z')\leq\varepsilon} f(z')\right]-\delta
    \end{align*}
Consequently, we deduce the expected result of the Theorem.
\end{proof}
    

When the problem is a classification problem (i.e., $\ZZ=\XX\times\YY$ with $\YY = [K]$), one can replace $f$ with  $\loss(f(x),y)$ with $\loss$ a measurable loss function and set the cost $c$ equals to:
\begin{align*}
    c((x,y),(x',y')) := \left\{
        \begin{array}{ll}
            d(x,x') & \mbox{if } y = y'\\
            +\infty & \mbox{otherwise.}
        \end{array}
    \right.
\end{align*} 
Hence, we recover the Adversarial classification problem using a Wasserstein-$\infty$ DRO problem. We will see in Chapter~\ref{xxx}, the geometric and topological properties of this set. 


\subsection{Game Theory and Adversarial Learning}

While adversarial classification can be naturally understood as a game between the attacker and the classifier, it has only been very recent that the problem has been studied from a game theoretic perspective. Adversarial examples have been studied under  the notions of Stackelberg game in~\cite{10.1145/2020408.2020495}, and  zero-sum game in~\cite{7533509,DBLP:journals/corr/abs-1906-02816,bose2021adversarial}.


In~\citep{bose2021adversarial}, the authors consider a setting with a convex loss function, a convex set of deterministic classifiers and a generative attacker. In this setting the authors show there is no duality gap for the game between the attacker and the learner. However, this setting is limited due to these assumptions. As we will see in Chapter~\ref{chap:calibration}, one can prove that no convex loss can be a good surrogate for the $0/1$ loss in the adversarial setting. Moreover, a generative attacker is weaker than an attacker that can move each point independently.

\cite{pinot2020randomization} proposed to study the adversarial game when the attacker is deterministic and regularized. The authors considers two different regularizations: the first one penalizes the average perturbation for the attacker and the second one penalizes the attacker if he attacks ``too many points''. In this setting,  the authors show that, under mild conditions, the risk for randomized classifiers is stricly smaller than the risk for deterministic classifiers. In particular, in this setting, the authors state the non-existence of Nash Equilibria when the classifier play a deterministic strategy and the attacker is regularized. 

%\textbf{Game Theory.} Some works previously casted the the adversarial risk minimization problem as a two-player game, either using the notion of Stackelberg game~\cite{10.1145/2020408.2020495}, or the concept of zero-sum game~\cite{7533509,DBLP:journals/corr/abs-1906-02816,bose2021adversarial}. These works usually study very restricted versions of the game where the players can be reduced to a convex and compact set a real-valued parameters and where assumptions on the loss function are very strong (e.g. convexity). In that sense, we can prove that when the loss is convex and the set $\Theta$ is convex, the duality gap is zero for deterministic classifiers (see Appendix~\ref{...} for a precise statement).
% \begin{thm}Let $\PP\in\mathcal{M}^1_+(\mathcal{X}\times\mathcal{Y})$, $\varepsilon>0$ and $\Theta$ a convex set.
% Let us assume that for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$, $l(\cdot,(x,y))$ is a convex function, for all $\theta\in\Theta$, $l(\theta,\cdot)$ is an upper semi-continuous function and for all $\theta\in\Theta$, $l(\theta,\cdot)\in L^1(\PP)$. Then we have the following:
% \begin{align*}
% \inf_{\theta\in\Theta} \sup_{\QQ\in \mathcal{A}_{\varepsilon}(\PP)} \mathbb{E}_{ \QQ }\left[l(\theta,(x,y))\right]
% =
% \sup_{\QQ\in \mathcal{A}_{\varepsilon}(\PP)}\inf_{\theta\in \Theta}  \mathbb{E}_{\QQ }\left[l(\theta,(x,y))\right]
% \end{align*}
% The supremum is always attained. If $\Theta$ is a compact set then, the infimum is also attained.
% \end{thm}
However, as we will see in Chapter~\ref{chap:calibration}, one can prove that no convex loss can be a good surrogate for the $0/1$ loss in the adversarial setting~\citep{pmlr-v125-bao20a,pmlr-v97-cranko19a,xxx}, narrowing the scope of this result. ~\cite{pinot2020randomization} studied partly this question for regularized deterministic adversaries, leaving the general setting of  randomized adversaries and  mixed equilibria unanswered, which is the scope of Chapter~\ref{chap:game}.


