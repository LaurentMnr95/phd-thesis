\chapter{Certification Methods for Adversarial Examples}
\minitoc
\label{chap:certification}
In this chapter we focus on noise injection and convex flows to build certifiable defenses on adversarial robustness. A desirable property for a function to be robust against adversarial examples is that the classifier $\mathbf{f}=(f_1,\dots,f_K)$ is a $M$-Lipschitz function with regards to Euclidean norm i.e. $\lVert \mathbf{f}(x)-\mathbf{f}(x')\rVert_2\leq M\lVert x-x'\rVert$ for all $x,x'\in\mathcal{X}$. 

\begin{prop}[\citep{li2019preventing}]Let $\mathbf{f}$ be a  $M$-Lipschitz classifier. Then for $\varepsilon>0$ and for every $x$ with label $y$ such that \begin{align*}
    \max(0,f_y(x)-\max_{y'\neq y}f_{y'}(x))> \sqrt{2}M\varepsilon
\end{align*}
then we have for every $\tau$ such that $\lVert \tau\rVert\leq \varepsilon$:
\begin{align*}
    \argmaxB_{i}f_i(x+\tau)=y
\end{align*}
\end{prop}

This property ensures robustness guarantees for Lipschitz classifiers
\input{sections/4_certification/noise_injection}
\input{sections/4_certification/resnet}
