\section{Discussions and Open questions}
In this chapter, we presented a new generic method to build $1$-Lipschitz layers.
We leverage the continuous time dynamical system interpretation of Residual Networks and show that using convex potential flows naturally defines $1$-Lipschitz neural networks.
After proposing a parametrization based on Input Convex Neural Networks~\citep{amos2017input}, we  show that our models  reach competitive results in classification and robustness in comparison which other existing $1$-Lipschitz approaches.
We also experimentally show that our layers provide scalable approaches without further regularization tricks to train very deep architectures.

Exploiting the ResNet architecture for devising flows have been an important research topic.
For example, in the context of generative modeling, Invertible Neural Networks~\citep{behrmann2019invertible} and Normalizing Flows~\citep{rezende2015variational, verine2021expressivity} are both import research topic.
More recently, Sylvester Normalizing Flows~\citep{vdberg2018sylvester} or Convex Potential Flows~\citep{huang2021convex} have had similar ideas to this present work but for a very different setting and applications. In particular, they did not have interest in the contraction property of convex flows and the link with adversarial robustness have been under-exploited.

% \paragraph{Exploiting other discretization schemes.} 
% Propoisition~\ref{prop:continuous-lip} gives a general argument to build $1$-Lipschitz Networks. We exploited some discretization schemes to build $1$-Lipschitz layers. One can imagine exploiting other  more complex discretization schemes as the implicit schemes, leap frog schemes. 

\paragraph{Expressivity of discretized convex potential flows.}
Propoisition~\ref{prop:continuous-lip} suggests to constraint the symmetric part of the Jacobian of $F_t$. We proposed to decompose $F_t$ as a sum of potential gradient and skew symmetric matrix. Finding other parametrizations is an open challenge.
Our models may not express all $1$-Lipschitz functions.
Knowing which functions can be approximated by our CPL layers is difficult even in the linear case. Indeed, let us define $\mathcal{S}_1(\RR^{d\times d})$ the space of real symmetric matrices with singular values bounded by $1$. Let us also define $\mathcal{M}_1(\RR^{d\times d})$ the space of real matrices with singular values bounded by $1$ in absolute value.
Let $\mathcal{P}(\RR^{d\times d})=\{A\in\RR^{d\times d}|\exists n\in \mathbb{N},S_1,\dots,S_n\in \mathcal{S}_1(\RR^d\times d)\text{ s.t. } A = S_1\dots S_n\}$. Then one can prove\footnote{A proof and justification of this result can be found here: \url{https://mathoverflow.net/questions/60174/factorization-of-a-real-matrix-into-hermitian-x-hermitian-is-it-stable}} that $\mathcal{P}(\RR^{d\times d}) \neq \mathcal{M}_1(\RR^{d\times d})$. Thus there exists $A\in\mathcal{M}_1(\RR^{d\times d})$ such that for all matrices $n$, for all matrices $S_1,\dots,S_n\in\mathcal{S}_1(\RR^{d\times d})$ such that $M\neq S_1,\dots,S_n$. Applied to the expressivity of discretized convex potential flows, the previous result means that there exists a $1$-Lipschitz linear function that cannot be approximated as a discretized flow of any depth of convex linear $1$-smooth potential flows as in Proposition~\ref{prop:discrete_convex_potentials}. Indeed such a flow would write: $x\mapsto\prod_i(1-2S_i)x$ where $S_i$ are symmetric matrices whose eigenvalues are in $[0,1]$, in other words such transformations are exactly described by $x\mapsto Mx$  for some $M\in\mathcal{P}(\RR^{d\times d})$. This is  an important question that requires further investigation. 


\paragraph{Going beyond ResNets} One can also think of extending  our work by the study of  other dynamical systems. Recent architectures such as Hamiltonian Networks~\citep{greydanus2019hamiltonian} and Momentum Networks~\citep{sander2021momentum} exhibit interesting properties and it worth digging into these architectures to build Lipschitz layers.
Finally, we hope to use similar approaches to build robust Recurrent Neural Networks~\citep{sherstinsky2020fundamentals} and Transformers~\citep{vaswani2017attention}. For Transformers,~\citet{xxx} has proposed a dynamical system interpretation of a flow on particles (i.e. the words in the the initial sentence). This can be seen as an interacting flow over a distributions. The question of robustness and Lipschitzness is way more technical since it implies Lipshitzness in the space of a distribution. One could imagine to use optimal transport~\citep{villani2003topics} and Wasserstein Gradient flows~\citep{ambrosio2005gradient} as tools for deriving Lipschitz guarantees for Transformers.


