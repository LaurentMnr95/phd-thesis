

\section{Introduction}

Modern neural networks have been known to be sensible against small, imperceptible and adversarially-chosen perturbations of their inputs~\citep{biggio2013evasion,szegedy2014intriguing}.
This vulnerability  has become a major issue as more and more neural networks have been deployed into production applications.
Over the past decade, the research progress plays out like a cat-and-mouse game between the development of more and more powerful attacks~\citep{goodfellow2014explaining,kurakin2016adversarial,carlini2017adversarial,croce2020reliable} and the design of empirical defense mechanisms~\citep{madry2017towards,moosavi2019robustness,cohen2019certified}.
Finishing the game calls for certified adversarial robustness~\citep{raghunathan2018certified,wong2018scaling}.
While recent work devised defenses with theoretical guarantees against adversarial perturbations, they share the same limitation, \ie, the tradeoffs between expressivity and robustness, and between scalability and accuracy.

A natural approach to provide robustness guarantees on a classifier is to enforce Lipschitzness properties. 
To achieve such properties, researchers mainly focused on two different kinds of approaches.
The first one is based on randomization~\citep{lecuyer2018certified,cohen2019certified,pinot2019theoretical} and consists in convolving the input with with a predefined probability distribution.
While this approach offers some level of scalability (\ie, currently the only certified defense on the ImageNet dataset), it suffers from significant impossibility results~\cite{yang2020randomized}.
A second approach consists in building $1$-Lipschitz layers using specific linear transform~\citep{cisse2017parseval,li2019preventing,anil2019sorting,trockman2021orthogonalizing,skew2021sahil,li2019preventing,singla2021householder}.
Knowing the Lipschitz constant of the network, it is then possible to compute a certification radius around any points. 

A large line of work explored the interpretation of residual neural networks \cite{he2016deep} as a parameter estimation problem of nonlinear dynamical systems~\citep{haber2017stable,e17Proposal,lu18beyond}.
Reconsidering the ResNet architecture as an Euler discretization of a continuous dynamical system yields to the trend around Neural Ordinary Differential Equation~\citep{chen2018neural}.
For instance, in the seminal work of~\citet{haber2017stable}, the continuous formulation offers more flexibility to investigate the stability of neural networks during inference, knowing that the discretization will be then implemented by the architecture design.
The notion of stability, in our context, quantifies how a small perturbation on the initial value impacts the trajectories of the dynamical system. 

From this continuous and dynamical interpretation, we  analyze the Lipschitzness property of Neural Networks. We then study the discretization schemes that can preserve the Lipschitzness properties. With this point of view, we can readily recover several previous methods that build 1-Lipschitz neural networks~\citep{trockman2021orthogonalizing,skew2021sahil}.
Therefore, the dynamical system perspective offers a general and flexible framework to build Lipschitz Neural Networks facilitating the discovery of new approaches.
In this vein, we introduce convex potentials in the design of the Residual Network flow and show that this choice of parametrization yields to by-design $1$-Lipschitz neural networks.
At the very core of our approach lies a new $1$-Lipschitz non-linear operator that we call {\em Convex Potential Layer} which allows us to adapt convex potential flows to the discretized case. 
These blocks enjoy the desirable property of stabilizing the training of the neural network by controlling the gradient norm, hence overcoming the exploding gradient issue.
We experimentally demonstrate our approach by training large-scale neural networks on several datasets, reaching state-of-the art results in terms of under-attack and certified accuracy.

