\chapter{Game Theory of Adversarial Examples}
\label{chap:game}
\minitoc

In this chapter, we answer \textbf{Question 1: ``What is the nature of equilibria in the adversarial examples game?''} by proving the existence of Mixed Nash equilibria in the adversarial example game when both the adversary and the classifier can use randomized strategies. First, we motivate in Section~\ref{sec:adv-problem} the necessity for using randomized strategies both with the attacker and the classifier. Then, we extend the work of~\cite{pydi2019adversarial}, by rigorously reformulating the adversarial risk as a linear optimization problem over distributions. In fact, we cast the adversarial risk minimization problem as a Distributionally Robust Optimization (DRO)~\citep{blanchet2019quantifying} problem for a well suited cost function. This formulation naturally leads us, in Section~\ref{sec:nash-eq}, to analyze adversarial risk minimization as a zero-sum game. We demonstrate that, in this game, the duality gap always equals $0$, meaning that it always admits approximate mixed Nash equilibria.  

Afterwards, we aim at designing an efficient algorithm to learn an optimally robust randomized classifier.
We focus on learning a finite mixture of classifiers. Drawing inspiration from robust optimization~\cite{sinha2017certifying} and subgradient methods~\cite{boyd2003subgradient}, we derive in Section~\ref{sec:algo} a first oracle algorithm to optimize a finite mixture. Then, following the line of work of~\citep{cuturi2013sinkhorn}, we introduce an entropic regularization to effectively compute an approximation of the optimal mixture. We validate our findings with experiments on simulated and real  datasets, namely CIFAR-10 an CIFAR-100~\cite{krizhevsky2009learning}.





\input{sections/2_game/adversarial_problem}
\input{sections/2_game/finding_optimal}
\input{sections/2_game/experiments}
\input{sections/2_game/lemma_measure_theory}