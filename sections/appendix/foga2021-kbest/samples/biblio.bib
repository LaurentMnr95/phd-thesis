@misc{zenodo,
  author       = {Laurent Meunier and
                  Herilalaina Rakotoarison and
                  Pak Kan Wong and
                  Baptiste Roziere and
                  Jérémy Rapin and
                  Olivier Teytaud and
                  Antoine Moreau and
                  Carola Doerr},
  title        = {{Black-Box Optimization Revisited: Improving 
                   Algorithm Selection Wizards through Massive
                   Benchmarking}},
  month        = may,
  year         = 2021,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.4744977},
  url          = {https://doi.org/10.5281/zenodo.4744977}
}


@inproceedings{AugerST05,
  author    = {Anne Auger and
               Marc Schoenauer and
               Olivier Teytaud},
  title     = {Local and global order 3/2 convergence of a surrogate evolutionary
               algorithm},
  booktitle = {Proc. of Genetic and Evolutionary Computation Conference (GECCO'05)},
  pages     = {857--864},
  year      = {2005},
  publisher  = {ACM},
  url       = {https://doi.org/10.1145/1068009.1068154},
  doi       = {10.1145/1068009.1068154}
}
@misc{lgrs-arxiv,
      title={Learning to Guide Random Search}, 
      author={Ozan Sener and Vladlen Koltun},
      year={2020},
      eprint={2004.12214},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{lgrs,
  author    = {Ozan Sener and
               Vladlen Koltun},
  title     = {Learning to Guide Random Search},
  booktitle = {Proc. of 8th International Conference on Learning Representations (ICLR)},
  year      = {2020},
  publisher = {OpenReview.net},
  url       = {https://openreview.net/forum?id=B1gHokBKwS},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SenerK20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{xavier,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics},
    year = {2010}
}
@Misc{BOpython,
    author = {Fernando Nogueira},
    title = {{Bayesian Optimization}: Open source constrained global optimization tool for {Python}},
    year = {2014--},
    url = " https://github.com/fmfn/BayesianOptimization"
}

@inproceedings{turbo,
 author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {5496--5507},
 publisher = {Curran Associates},
 title = {Scalable Global Optimization via Local {B}ayesian Optimization},
 unusedurl = {https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{nacim,
author = {Belkhir, Nacim and Dr\'{e}o, Johann and Sav\'{e}ant, Pierre and Schoenauer, Marc},
title = {Per Instance Algorithm Configuration of CMA-ES with Limited Budget},
year = {2017},
unusedisbn = {9781450349208},
publisher = {ACM},
unusedurl = {https://doi.org/10.1145/3071178.3071343},
unuseddoi = {10.1145/3071178.3071343},
booktitle = {Proc. of Genetic and Evolutionary Computation Conference (GECCO'17)},
pages = {681–688}
}


@inproceedings{astnew,
  author    = {Anne Auger and
               Marc Schoenauer and
               Olivier Teytaud},
  title     = {Local and global order 3/2 convergence of a surrogate evolutionary
               algorithm},
  booktitle = {Proc. of Genetic and Evolutionary Computation Conference (GECCO'05)},
  pages     = {857--864},
  year      = {2005},
  publisher = {ACM},
  unusedunusedurl = {http://doi.acm.org/10.1145/1068009.1068154},
  unusedodi= {10.1145/1068009.1068154},
  timestamp = {Fri, 10 Feb 2006 16:05:52 +0100},
  biburl    = {http://dblp.org/rec/bib/conf/gecco/AugerST05},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{
lmrs,
title={Learning to Guide Random Search},
author={Ozan Sener and Vladlen Koltun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1gHokBKwS}
}
@inproceedings{added1, title={A computationally efficient limited memory CMA-ES for large scale optimization}, author={Loshchilov, Ilya}, booktitle={Proc. of Genetic and Evolutionary Computation (GECCO'14)}, pages={397--404}, year={2014} } 
@inproceedings{added2, 
title={Projection-based restricted covariance matrix adaptation for high dimension}, 
author={Akimoto, Youhei and Hansen, Nikolaus}, 
booktitle={Proc. of Genetic and Evolutionary Computation (GECCO'16)}, 
pages={197--204}, year={2016} } 
@article{added3, title={Large scale black-box optimization by limited-memory matrix adaptation}, 
author={Loshchilov, Ilya and Glasmachers, Tobias and Beyer, Hans-Georg}, 
journal={IEEE Transactions on Evolutionary Computation}, volume={23}, number={2}, pages={353--358}, 
year={2018}, publisher={IEEE} }

@inproceedings{AIplanning,
	author={Mauro Vallati and Frank Hutter and Luk{\'{a}}s Chrpa and Thomas Leo McCluskey},
  title     = {On the Effective Configuration of Planning Domain Models},
  booktitle = {Proc. of International Joint Conference on Artificial Intelligence ({IJCAI}'15)},
    year      = {2015}
}

@inproceedings{ppsnrescaling,
  title={Variance reduction for better sampling in continuous domains},
  author={Meunier, Laurent and Doerr, Carola and Rapin, Jeremy and Teytaud, Olivier},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={154--168},
  year={2020},
  organization={Springer}
}

@article{bach,
  title={Finding Global Minima via Kernel Approximations},
  author={Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
  journal={arXiv preprint arXiv:2012.11978},
  year={2020}
}

@inproceedings{
lrms,
title={Learning to Guide Random Search},
author={Ozan Sener and Vladlen Koltun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1gHokBKwS}
}
@misc{dash, 
author = {Jeremy Rapin and Olivier Teytaud}, 
year = {2020}, 
title = {Dashboard of Results for {N}evergrad Platform}, 
howpublished = {\url{https://dl.fbaipublicfiles.com/nevergrad/allxps/list.html}}
}
@misc{source, 
title = {Anonymized gitHub repository with all source code of our work}, 
author = {Anonymous}, 
year = {2020}, 
howpublished = {\url{https://anonymous.4open.science/r/5118cc2c-e2b0-475f-ba8d-3a2971a93acd/}}}

@article{icmldoe,
  title={Fully Parallel Hyperparameter Search: Reshaped Space-Filling},
  author={Cauwet, Marie-Liesse and Couprie, Camille and Dehos, Julien and Luc, Pauline and Rapin, J{\'e}r{\'e}my and Riviere, Morgane and Teytaud, Fabien and Teytaud, Olivier},
  journal={arXiv preprint arXiv:1910.08406. To appear in Proc. of {ICML} 2020},
  year={2019}
}

@misc{nevergrad,
author = {Jeremy Rapin and Olivier Teytaud},
title = {{Nevergrad - A gradient-free optimization platform}},
year = {2018},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://GitHub.com/FacebookResearch/Nevergrad}},
}

@inproceedings{ppsnkbest,
  author    = {Laurent Meunier and
               Yann Chevaleyre and
               J{\'{e}}r{\'{e}}my Rapin and
               Cl{\'{e}}ment W. Royer and
               Olivier Teytaud},
  editor    = {Thomas B{\"{a}}ck and
               Mike Preuss and
               Andr{\'{e}} H. Deutz and
               Hao Wang and
               Carola Doerr and
               Michael T. M. Emmerich and
               Heike Trautmann},
  title     = {On Averaging the Best Samples in Evolutionary Computation},
  booktitle = {Parallel Problem Solving from Nature - {PPSN} {XVI} - 16th International
               Conference, {PPSN} 2020, Leiden, The Netherlands, September 5-9, 2020,
               Proceedings, Part {II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12270},
  pages     = {661--674},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58115-2\_46},
  doi       = {10.1007/978-3-030-58115-2\_46},
  timestamp = {Fri, 04 Sep 2020 10:49:42 +0200},
  biburl    = {https://dblp.org/rec/conf/ppsn/MeunierCRRT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sumo,
  author    = {Fabien Teytaud and
               Olivier Teytaud},
  title     = {Why one must use reweighting in estimation of distribution algorithms},
  booktitle = {Genetic and Evolutionary Computation Conference, {GECCO} 2009, Proceedings,
               Montreal, Qu{\'{e}}bec, Canada, July 8-12, 2009},
  pages     = {453--460},
  year      = {2009},
  uuucrossref  = {DBLP:conf/gecco/2009g},
  unuurl       = {https://doi.org/10.1145/1569901.1569964},
  uuiuunuseddoi       = {10.1145/1569901.1569964},
}

@article{amorales,
title={Evolution Strategies},
author={H.J. Escalante and A. Morales Reyes},
journal={CCC-INAOE tutorial},
year={2013}
}

@InProceedings{cmsa,
author="Beyer, Hans-Georg
and Sendhoff, Bernhard",
randomeditor="Rudolph, G{\"u}nter
and Jansen, Thomas
and Beume, Nicola
and Lucas, Simon
and Poloni, Carlo",
title="Covariance Matrix Adaptation Revisited -- The CMSA Evolution Strategy --",
booktitle="Parallel Problem Solving from Nature -- PPSN X",
year="2008",
publisher="Springer Berlin Heidelberg",
unusedaddress="Berlin, Heidelberg",
pages="123--132",
abstract="The covariance matrix adaptation evolution strategy (CMA-ES) rates among the most successful evolutionary algorithms for continuous parameter optimization. Nevertheless, it is plagued with some drawbacks like the complexity of the adaptation process and the reliance on a number of sophisticatedly constructed strategy parameter formulae for which no or little theoretical substantiation is available. Furthermore, the CMA-ES does not work well for large population sizes. In this paper, we propose an alternative -- simpler -- adaptation step of the covariance matrix which is closer to the ``traditional'' mutative self-adaptation. We compare the newly proposed algorithm, which we term the CMSA-ES, with the CMA-ES on a number of different test functions and are able to demonstrate its superiority in particular for large population sizes.",
unusedisbn="978-3-540-87700-4"
}

@article{beyerbenefitofsex,
author = {Beyer, Hans-Georg},
title = {Toward a Theory of Evolution Strategies: On the Benefits of Sex---the $(\mu/\mu,\lambda)$ Theory},
year = {1995},
issue_date = {Spring 1995},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco.1995.3.1.81},
doi = {10.1162/evco.1995.3.1.81},
abstract = {The multirecombinant (μ/μ, λ) evolution strategy (ES) is investigated for real-valued, N-dimensional parameter spaces. The analysis includes both intermediate recombination and dominant recombination, as well. These investigations are done for the spherical model first. The problem of the optimal population size depending on the parameter space dimension N is solved. A method extending the results obtained for the spherical model to nonspherical success domains is presented.The power of sexuality is discussed and it is shown that this power does not stem mainly from the “combination” of “good properties” of the mates (building block hypothesis) but rather from genetic repair diminishing the influence of harmful mutations. The dominant recombination is analyzed by introduction of surrogate mutations leading to the concept of species. Conclusions for evolutionary algorithms (EAs), including genetic algorithms (GAs), are drawn.},
journal = {Evol. Comput.},
month = mar,
pages = {81–111},
numpages = {31},
keywords = {building block hypothesis, genetic variety versus genetic repair, multimembered evolution strategies (ES), multirecombinant GA, multirecombination: intermediate and dominant/discrete, nonspherical success domains, progress rate theory, benefits of sexuality}
}
@inproceedings{jeb,
  TITLE = {{Log-linear Convergence of the Scale-invariant $(\mu/\mu_{w},\lambda)$-{ES} and Optimal $\mu$ for Intermediate Recombination for Large Population Sizes}},
  AUTHOR = {Jebalia, Mohamed and Auger, Anne},
  URL = {https://hal.inria.fr/inria-00494478},
  BOOKTITLE = {{Parallel Problem Solving From Nature (PPSN2010)}},
  ADDRESS = {Krakow, Poland},
  EDITOR = {Robert Schaefer and Carlos Cotta and Joanna Kolodziej and G{\"u}nter Rudolph},
  PUBLISHER = {{Springer}},
  SERIES = {Lecture Notes in Computer Science},
  PAGES = {xxxx-xxx},
  YEAR = {2010},
  MONTH = Sep,
  PDF = {https://hal.inria.fr/inria-00494478/file/ppsn2010JebaliaAuger.pdf},
  HAL_ID = {inria-00494478},
  HAL_VERSION = {v1},
}
@phdthesis{islands,
author = {Skolicki, Zbigniew Maciej},
advisor = {Jong, Kenneth A.},
title = {An Analysis of Island Models in Evolutionary Computation},
year = {2007},
isbn = {9780549329619},
publisher = {George Mason University},
address = {USA},
abstract = {Island models (IMs) are a class of distributed evolutionary algorithms (EAs) in which the population is split into multiple sub-populations called islands. Separate EAs run independently on each island, but they interact by means of migrating individuals. Therefore, IMs are different both from a single-population standard EA, as well as from a set of multiple isolated EAs. IMs are interesting for several reasons. They have been reported to yield better results than standard EAs. IMs are also advantageous when computational tasks must be distributed across multiple machines because their structure is easy to parallelize. However, despite many studies, no comprehensive theory describing their behavior has been developed. Due to the lack of theory and a complex architecture with many control parameters, setting up IMs has been a trial-and-error process, guided mostly by “rules of thumb.” In this dissertation, I adopt a two-level (intra- and inter-island) view of IMs and show how this approach makes understanding their dynamics easier. They behave very differently than standard EAs, and in order to take full advantage of this, I propose a better utilization of the inter-island level of evolution. In particular, I argue for setups with many relatively small islands, and I also show that compositional evolution may scale to the inter-island level. The two levels of evolution influence each other, and I analyze this interaction more deeply. Migrations profoundly change the local dynamics and stimulate evolution, which often ultimately results in better performance. I study the role of genetic operators in this behavior and also create mathematical models of after-migration dynamics. This analysis gives us a better understanding of mixing and the survival of genes locally, and these processes in turn determine the type and level of interaction between islands globally. Further, using island heterogeneity enhances the inter-island evolution. Following the study, I analyze IM behavior on a range of test problems, including two complex domains. This dissertation improves our understanding of the dynamics of IMs and suggests a qualitative change in the way we think about them. This perspective offers new guidelines for configuring IM parameters and opens new directions for future work.},
note = {AAI3289714}
}
@inproceedings{chooselambda,
  TITLE = {{Conditioning, halting criteria and choosing lambda}},
  AUTHOR = {Teytaud, Olivier},
  URL = {https://hal.inria.fr/inria-00173237},
  BOOKTITLE = {{EA07}},
  ADDRESS = {Tours, France},
  YEAR = {2007},
  KEYWORDS = {halting criteria ; theory ; optimization ; evolutionary algorithms},
  PDF = {https://hal.inria.fr/inria-00173237/file/chooselambda.pdf},
  HAL_ID = {inria-00173237},
  HAL_VERSION = {v1},
}
@inproceedings{anneweights,
author = {Auger, Anne and Brockhoff, Dimo and Hansen, Nikolaus},
title = {Mirrored Sampling in Evolution Strategies with Weighted Recombination},
year = {2011},
isbn = {9781450305570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001576.2001694},
doi = {10.1145/2001576.2001694},
abstract = {This paper introduces mirrored sampling into evolution strategies (ESs) with weighted multi-recombination. Two further heuristics are introduced: pairwise selection selects at most one of two mirrored vectors in order to avoid a bias due to recombination. Selective mirroring only mirrors the worst solutions of the population. Convergence rates on the sphere function are derived that also yield upper bounds for the convergence rate on any spherical function. The optimal fraction of offspring to be mirrored is regardless of pairwise selection one without selective mirroring and about 19\% with selective mirroring, where the convergence rate reaches a value of 0.390. This is an improvement of 56\% compared to the best known convergence rate of 0.25 with positive recombination weights.},
booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
pages = {861–868},
numpages = {8},
keywords = {mirroring, weighted recombination, evolution strategies},
location = {Dublin, Ireland},
series = {GECCO '11}
}
@inproceedings{arnoldweights,
author = {Arnold, Dirk V. and Beyer, Hans-Georg and Melkozerov, Alexander},
title = {On the Behaviour of Weighted Multi-Recombination Evolution Strategies Optimising Noisy Cigar Functions},
year = {2009},
isbn = {9781605583259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1569901.1569969},
doi = {10.1145/1569901.1569969},
abstract = {Cigar functions are convex quadratic functions that are characterised by the presence of only two distinct eigenvalues of their Hessian, the smaller one of which occurs with multiplicity one. Their ridge-like topology makes them a useful test case for optimisation strategies. This paper extends previous work on modelling the behaviour of evolution strategies with isotropically distributed mutations optimising cigar functions by considering weighted recombination as well as the effects of noise on optimisation performance. It is found that the same weights that have previously been seen to be optimal for the sphere and parabolic ridge functions are optimal for cigar functions as well. The influence of the presence of noise on optimisation performance depends qualitatively on the trajectory of the search point, which in turn is determined by the strategy's mutation strength as well as its population size and recombination weights. Analytical results are obtained for the case of cumulative step length adaptation.},
booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
pages = {483–490},
numpages = {8},
keywords = {noise, weighted recombination, cigar function, cumulative step length adaptation, evolution strategy},
location = {Montreal, Qu\'{e}bec, Canada},
series = {GECCO '09}
}
@misc{sm4,
      title={One-Shot Decision-Making with and without Surrogates}, 
      author={Jakob Bossek and Pascal Kerschke and Aneta Neumann and Frank Neumann and Carola Doerr},
      year={2019},
      eprint={1912.08956},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{sm3,
      title={A Novel Surrogate-assisted Evolutionary Algorithm Applied to Partition-based Ensemble Learning}, 
      author={Arkadiy Dushatskiy and Tanja Alderliesten and Peter A. N. Bosman},
      year={2021},
      eprint={2104.08048},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{sm2,
      title={Meta-models for structural reliability and uncertainty quantification}, 
      author={Bruno Sudret},
      year={2012},
      eprint={1203.2062},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}
@misc{sm1,
      title={Improving Surrogate Model Accuracy for the LCLS-II Injector Frontend Using Convolutional Neural Networks and Transfer Learning}, 
      author={Lipi Gupta and Auralee Edelen and Nicole Neveu and Aashwin Mishra and Christopher Mayes and Young-Kee Kim},
      year={2021},
      eprint={2103.07540},
      archivePrefix={arXiv},
      primaryClass={physics.acc-ph}
}

@misc{monoto,
      title={Global Linear Convergence of Evolution Strategies on More Than Smooth Strongly Convex Functions},
      author={Youhei Akimoto and Anne Auger and Tobias Glasmachers and Daiki Morinaga},
      year={2020},
      eprint={2009.08647},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@inproceedings{clop,
  author    = {R{\'{e}}mi Coulom},
  editor    = {H. Jaap van den Herik and
               Aske Plaat},
  title     = {{CLOP:} Confident Local Optimization for Noisy Black-Box Parameter
               Tuning},
  booktitle = {Advances in Computer Games - 13th International Conference, {ACG}
               2011, Tilburg, The Netherlands, November 20-22, 2011, Revised Selected
               Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {7168},
  pages     = {146--157},
  publisher = {Springer},
  year      = {2011},
  url       = {https://doi.org/10.1007/978-3-642-31866-5\_13},
  doi       = {10.1007/978-3-642-31866-5\_13},
  timestamp = {Tue, 14 May 2019 10:00:53 +0200},
  biburl    = {https://dblp.org/rec/conf/acg/Coulom11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{noisymesh,
  author  = {Charles Audet and Amina Ihaddadene and S{\'{e}}bastien {Le Digabel} and Christophe Tribes},
  title   = {Robust optimization of noisy blackbox problems using the Mesh Adaptive Direct Search algorithm},
  journal = {Optimization Letters},
  year    = {2018},
  volume  = {12},
  number  = {4},
  pages   = {675--689},
  doi     = {10.1007/s11590-017-1226-6},
  url    = {https://scholar.google.com/scholar?cluster=17486798405551999711},
}

@inproceedings{astete,
author = {Astete-Morales, Sandra and Cauwet, Marie-Liesse and Teytaud, Olivier},
title = {Evolution Strategies with Additive Noise: A Convergence Rate Lower Bound},
year = {2015},
isbn = {9781450334341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2725494.2725500},
doi = {10.1145/2725494.2725500},
abstract = {We consider the problem of optimizing functions corrupted with additive noise. It is known that Evolutionary Algorithms can reach a Simple Regret O(1/sqrt n) within logarithmic factors, when n is the number of function evaluations. Here, Simple Regret at evaluation $n$ is the difference between the evaluation of the function at the current recommendation point of the algorithm and at the real optimum. We show mathematically that this bound is tight, for any family of functions that includes sphere functions, at least for a wide set of Evolution Strategies without large mutations.},
booktitle = {Proceedings of the 2015 ACM Conference on Foundations of Genetic Algorithms XIII},
pages = {76–84},
numpages = {9},
keywords = {additive noise, evolution strategies, noisy optimization},
location = {Aberystwyth, United Kingdom},
series = {FOGA '15}
}

@article{arnoldbeyer,
  title={A general noise model and its effects on evolution strategy performance},
  author={Arnold, Dirk V and Beyer, H-G},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={10},
  number={4},
  pages={380--391},
  year={2006},
  publisher={IEEE}
}



@article{dushatskiy2021novel,
  title={A novel surrogate-assisted evolutionary algorithm applied to partition-based ensemble learning},
  author={Dushatskiy, Arkadiy and Alderliesten, Tanja and Bosman, Peter AN},
  journal={arXiv preprint arXiv:2104.08048},
  year={2021}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization},
  author={Bergstra, James and Bengio, Yoshua},
  journal={JMLR},
  volume={13},
  number={Feb},
  year={2012}
}

@InProceedings{AST,
	author={Anne Auger and Marc Schoenauer and Olivier Teytaud},
	title={Local and global order 3/2 convergence of a Surrogate Evolutionnary Algorithm},
	booktitle={Gecco},
	year=2005,
	pages={8}
}

@misc{source_public, 
title = {Black-Box Optimization Revisited: Improving Algorithm Selection Wizards through Massive Benchmarking}, 
author = {Meunier, Laurent and Rakotoarison, Herilalaina and Wong, Pak Kan and Roziere, Baptiste and Rapin, Jeremy and Teytaud, Olivier and Moreau, Antoine and Doerr, Carola}, 
year = {2020}, 
howpublished = {\url{https://dl.fbaipublicfiles.com/nevergrad/all}}}

@misc{pbt,
    title={Population Based Training of Neural Networks},
    author={Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
    year={2017},
    eprint={1711.09846},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{noisybbobold,
author = {Hansen, Nikolaus and Auger, Anne and Finck, Steffen and Ros, Raymond},
year = {2009},
month = {01},
pages = {},
journal={Technical Report PPE, 2009/21},
title = {Real-Parameter Black-Box Optimization Benchmarking 2009: Experimental Setup}
}

@inproceedings{noisybbob,
  author    = {Anne Auger and
               Nikolaus Hansen},
  title     = {Benchmarking the {(1+1)-CMA-ES} on the {BBOB-2009} noisy testbed},
  booktitle = {Proc. of Genetic and Evolutionary Computation Conference ({GECCO}'09, Companion Material},
  pages     = {2467--2472},
  year      = {2009},
  publisher = {ACM},
  url       = {https://doi.org/10.1145/1570256.1570345},
  doi       = {10.1145/1570256.1570345},
  timestamp = {Tue, 06 Nov 2018 11:06:40 +0100},
  biburl    = {https://dblp.org/rec/conf/gecco/AugerH09a.bib}
}

@article{bbob-large-ASOC,
  author    = {Konstantinos Varelas and
               Ouassim Ait ElHara and
               Dimo Brockhoff and
               Nikolaus Hansen and
               Duc Manh Nguyen and
               Tea Tusar and
               Anne Auger},
  title     = {Benchmarking large-scale continuous optimizers: The {BBOB}-largescale
               testbed, a {COCO} software guide and beyond},
  journal   = {Appl. Soft Comput.},
  volume    = {97},
  number    = {Part},
  pages     = {106737},
  year      = {2020},
  url       = {https://doi.org/10.1016/j.asoc.2020.106737},
  doi       = {10.1016/j.asoc.2020.106737},
  timestamp = {Tue, 16 Mar 2021 15:26:36 +0100},
  biburl    = {https://dblp.org/rec/journals/asc/VarelasEBHNTA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{chaining,
  author={D. {Molina} and M. {Lozano} and F. {Herrera}},
  booktitle={2009 Ninth International Conference on Intelligent Systems Design and Applications}, 
  title={Memetic Algorithm with Local Search Chaining for Continuous Optimization Problems: A Scalability Test}, 
  year={2009},
  volume={},
  number={},
  pages={1068-1073},}
@misc{ars,
    title={Simple random search provides a competitive approach to reinforcement learning},
    author={Horia Mania and Aurelia Guy and Benjamin Recht},
    year={2018},
    eprint={1803.07055},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@ARTICLE{unitcommitment,
  author={Narayana Prasad {Padhy}},
  journal={IEEE Transactions on Power Systems}, 
  title={Unit commitment-a bibliographical survey}, 
  year={2004},
  volume={19},
  number={2},
  pages={1196-1205},}
  
@article{barry2020evolutionary,
  title={Evolutionary algorithms converge towards evolved biological photonic structures},
  author={Barry, Mamadou Aliou and Berthier, Vincent and Wilts, Bodo D and Cambourieux, Marie-Claire and Bennet, Pauline and Poll{\`e}s, R{\'e}mi and Teytaud, Olivier and Centeno, Emmanuel and Biais, Nicolas and Moreau, Antoine},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{ars,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}
@article{gaseparable,
  title={Re-evaluating genetic algorithm performance under coordinate rotation of benchmark functions. A survey of some theoretical and practical aspects of genetic algorithms},
  author={Salomon, Ralf},
  journal={BioSystems},
  volume={39},
  number={3},
  pages={263--278},
  year={1996},
  publisher={Elsevier}
}

@article{escompr,
 author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
 title = {Evolution Strategies –A Comprehensive Introduction},
 year = {2002},
 issue_date = {May 2002},
 publisher = {Kluwer Academic Publishers},
 unusedaddress = {USA},
 volume = {1},
 number = {1},
 sdfasfdsaissn = {1567-7818},
 url = {https://doi.org/10.1023/A:1015059928466},
 unuseddoi = {10.1023/A:1015059928466},
 journal = {Natural Computing: An International Journal},
 month = may,
 pages = {3–52},
 numpages = {50},
 keywords = {Darwinian evolution, evolution strategies, evolutionary computation, design principles for genetic operators, computational intelligence, optimization}
}

@Article{HAN,
  author =  {Nikolaus Hansen and Andreas Ostermeier},
  title =  {Completely Derandomized Self-Adaptation in Evolution Strategies},
  journal =  {Evolutionary Computation},
  year =  {2003},
  volume =  {11},
  number =  {1}
}

@article{fournierAlgorithmica,
	title = { {L}ower {B}ounds for {C}omparison {B}ased {E}volution {S}trategies using {VC}-dimension and {S}ign {P}atterns},
	author = {{F}ournier, {H}erv{\'e} and {T}eytaud, {O}livier},
	abstract = {{W}e derive lower bounds on the convergence rate of comparison based or selection based algorithms, improving existing results in the continuous setting, and extending them to non-trivial results in the discrete case. {T}his is achieved by considering the {VC}-dimension of the level sets of the fitness functions; results are then obtained through the use of the shatter function lemma. {I}n the special case of optimization of the sphere function, improved lower bounds are obtained by an argument based on the number of sign patterns.},
	keywords = {{E}volutionary {A}lgorithms;{P}arallel {O}ptimization;{C}omparison-based algorithms;{VC}-dimension;{S}ign patterns;{C}omplexity},
language = {{A}nglais},
affiliation = {{P}arall{\'e}lisme, {R}{\'e}seaux, {S}yst{\`e}mes d'information, {M}od{\'e}lisation - {PRISM} - {CNRS} : {UMR}8144 - {U}niversit{\'e} de {V}ersailles-{S}aint {Q}uentin en {Y}velines - {L}aboratoire de {R}echerche en {I}nformatique - {LRI} - {CNRS} : {UMR}8623 - {U}niversit{\'e} {P}aris {S}ud - {P}aris {XI} - {TAO} - {INRIA} {S}aclay - {I}le de {F}rance - {INRIA} - {CNRS} : {UMR}8623 - {U}niversit{\'e} {P}aris {S}ud - {P}aris {XI} },
	publisher = {{S}pringer },
	journal = {{A}lgorithmica },
	audience = {internationale },
    year = {2010},
}

@INPROCEEDINGS{quasiopposite,
author={S. {Rahnamayan} and H. R. {Tizhoosh} and M. M. A. {Salama}},
booktitle={2007 IEEE Congress on Evolutionary Computation},
title={Quasi-oppositional Differential Evolution},
year={2007},
volume={},
number={},
pages={2229-2236},
keywords={evolutionary computation;quasioppositional differential evolution;black-box optimization;global optimization algorithm;Acceleration;Pattern analysis;Machine intelligence;Genetic mutations;Biomedical engineering;Instruments;Benchmark testing;Robust control;Optimization methods;Convergence},
unuseddoi={10.1109/CEC.2007.4424748},
unusedISSN={},
month={Sep.},}

@unpublished{bousquet,
title={Critical Hyper-Parameters: No Random, No Cry},
author={Bousquet, Olivier and Gelly, Sylvain and  Karol, Kurach and Teytaud, Olivier and Vincent, Damien},
note={Preprint \url{https://arxiv.org/pdf/1706.03200.pdf}},
year={2017}
}

@InProceedings{beyerhellwignoise,
author="Hellwig, Michael
and Beyer, Hans-Georg",
randomeditor="Handl, Julia
and Hart, Emma
and Lewis, Peter R.
and L{\'o}pez-Ib{\'a}{\~{n}}ez, Manuel
and Ochoa, Gabriela
and Paechter, Ben",
title="Evolution Under Strong Noise: A Self-Adaptive Evolution Strategy Can Reach the Lower Performance Bound - The pcCMSA-ES",
booktitle="Parallel Problem Solving from Nature -- PPSN XIV",
year="2016",
publisher="Springer International Publishing",
unusedaddress="Cham",
pages="26--36",
abstract="According to a theorem by Astete-Morales, Cauwet, and Teytaud, ``simple Evolution Strategies (ES)'' that optimize quadratic functions disturbed by additive Gaussian noise of constant variance can only reach a simple regret log-log convergence slope                                                                                   {\$}{\$}{\backslash}ge -1/2{\$}{\$}                   (lower bound). In this paper a population size controlled ES is presented that is able to perform better than the                                                                                   {\$}{\$}-1/2{\$}{\$}                   limit. It is shown experimentally that the pcCMSA-ES is able to reach a slope of                                                                                   {\$}{\$}-1{\$}{\$}                   being the theoretical lower bound of all comparison-based direct search algorithms.",
isbn="978-3-319-45823-6"
}