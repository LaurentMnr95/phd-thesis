\section{Introduction}
We consider the problem of testing whether two variables $X$ and $Y$ are independent given a set of confounding variables $Z$, which can be formulated as a hypothesis testing problem of the form:
\begin{align*}
    H_0:~ X\perp Y | Z\qquad\text{vs.}\qquad H_1:X\not\perp Y | Z.
\end{align*}
Testing for conditional independence (CI) is central in a wide variety of statistical learning problems. For example, it is at the core of graphical modeling~\citep{lauritzen1996graphical,koller2009probabilistic}, causal discovery~\citep{pearl2009causal,glymour2019review}, variable selection \citep{candes2018panning}, dimensionality reduction~\citep{li2018sufficient}, and biomedical studies~\citep{richardson1993bayesian,dobra2004sparse,markowetz2007inferring}. 
 
Testing for $H_0$ in such applications is known to be a highly challenging task~\citep{gcm2020,neykov2021minimax}. A large line of work has focused on the design of measures for conditional dependence based for example on kernel methods~\cite{fukumizu2008kernel,Sheng2019OnDA,NEURIPS2020_f340f1b1,huang2020kernel} and rank statistics~\cite{azadkia2021simple,shi2021azadkia}. Testing for conditional independence is even a more difficult as it requires both designing a test statistic which measures the conditional dependencies and controlling its quantiles. Indeed, existing tests may fail to control the type-I error, especially when the confounding set of variables is high-dimensional with a complex dependency structure~\cite{bergsma2004testing}. Furthermore, even if the test is valid, the availability of limited data makes the problem of discriminating between the null and alternative hypotheses extremely difficult, resulting in a test of low power. These challenges has motivated the development of a series of practical methods attempting to reliably test for conditional independence. These include tests based on kernels ~\citep{zhang2012kernel,doran2014permutation,strobl2019approximate,Zhang2017FeaturetoFeatureRF}, ranks~\cite{runge2018conditional,mittag2018nonparametric}, models~\citep{sen2017modelpowered,sen2018mimic,chalupka2018fast,gcm2020}, permutations and  samplings~\citep{berrett2020conditional,candes2018panning,BellotS19, shi2021double,javanmard2021pearson},
and optimal transport~\cite{warren2021wasserstein}.



% In addition, estimating conditional independence  may become computationally expensive as the dimension of the problem increases. For example, the Kernel Conditional Independence Test (KCIT)~\citep{zhang2012kernel} requires a cubic number of operations with respect to the sample size. Moreover, in this setting, any bootstrap or permutation procedure becomes expensive as one needs to compute multiple times the statistic of interest. Such strategies are often used to estimate the p-value as the (asymptotic) null distribution, which may be parameter dependent~\citep{sen2017modelpowered,shi2021double}. In that context, \citet{strobl2019approximate} propose to reduce the computational complexity of KCIT by considering a Random Fourier features approximations of the kernels involved in the statistic, resulting in a test that scales linearly with respect to the sample size. 

% We explore in this work similar ideas in order to approximate the proposed statistic and obtain a linear time test for conditional independence.
% Kernel methods offer a general and successful framework for capturing (conditional) dependence of variables. However estimating conditional independence  may become computationally expensive as the dimensions of the problem increase. For example, the Kernel Conditional Independence Test (KCIT)~\citep{zhang2012kernel} scales at least quadratically with sample size. In \cite{strobl1702approximate}, the authors propose to reduce the time complexity of KCIT by considering a Random Fourier features approximations of the kernels involved in the statistic and obtain a test which scales linearly with respect to the sample size. We explore in this work similar ideas in order to approximate the  proposed statistic and obtain a linear time test for conditional independence.


In this paper, we propose a new kernel-based test for conditional independence with asymptotic theoretical guarantees.
% Given samples $(X,Y,Z)$ from the joint distribution $P_{XYZ}$, we aim at testing the independence of $X$ and $Y$ conditionally to $Z$. 
Taking inspiration from~\cite{chwialkowski2015fast,jitkrittum2017adaptive,NEURIPS2019_0e2db0cb}, we use the $\ell^p$ distance between two well-chosen analytic kernel mean embeddings evaluated at a finite set of locations. We show that this measure encodes the conditional dependence relation of the random
variables under study. 
% From this metric, we derive a new conditional dependence measure which can scale linearly with respect to the number of samples. 
Under common assumptions on the richness of the RKHS, we derive the asymptotic null distribution of our measure, and design a simple nonparametric test that is distribution-free under the null hypothesis. Furthermore, we show that our test is consistent. Lastly, we validate our theoretical claims and study the performance of the proposed approach using simulated conditionally (in)dependent data and show that our testing procedure outperforms state-of-the-art methods.


\subsection{Related Work}
% \cite{fukumizu2008kernel} was the first paper to introduce a kernel-based measure of conditional dependence. The authors presented the conditional cross-covariance operator as a way to characterize the conditional relation between $X$ and $Y$ given $Z$. They also showed how to estimate this operator, however, without offering a testing procedure that is based on it. Moreover, the asymptotic distribution of the statistic from \cite{fukumizu2008kernel} is unknown.


\citet{zhang2012kernel} propose a kernel based-test (KCIT), by leveraging the characterization of conditional independence derived in \citep{daudin1980partial} to form a test statistic. The authors of this work obtain the asymptotic null distribution of the proposed statistic and derived a practical procedure from it to test for $H_0$. However, one main practical issue of the proposed test is that the asymptotic null distribution of their statistic cannot be computed directly as it involved unknown quantities. To address this problem, the authors propose to approximate it either with Monte Carlo simulations or by fitting a Gamma distribution. In our work, we propose a new kernel-based statistic to test for conditional independence and show that its asymptotic null distribution is simply the standard normal distribution. In addition \citet{zhang2012kernel} extended the Gaussian process (GP) regression framework to the multi-output case, which allowed them to find the hyperparameters involved in the test statistic, maximizing the marginal likelihood. We also deploy a similar optimization procedure to that of \citet{zhang2012kernel}, however, in our case the output of the GP regression is univariate and therefore more computationally efficient.

% \citet{zhang2012kernel} proposed a kernel based-test (KCIT), by leveraging the characterization of conditional independence derived in \citep{daudin1980partial} to form a test statistic. The authors of this work analyzed the asymptotic distribution of the proposed statistic, offering a practical procedure to test for $H_0$. In addition, \citet{zhang2012kernel} extended the Gaussian process (GP) regression framework to the multi-output case, which allowed them to find the hyperparameters involved in the test statistic, maximizing the marginal likelihood. However, this test is computationally expensive since both the estimation of the p-value as well as the computation of the statistic require at least a cubic number of operations. In our work, we propose a new kernel-based statistic that is computationally efficient, which scales linearly with the number of samples. We also deploy a similar optimization procedure to that of \citet{zhang2012kernel}, however, in our case the output of the GP regression is univariate. In addition, the p-values of the proposed test are easy to obtain: we show that the asymptotic null distribution of our statistic follows the standard normal distribution.



Other CI tests proposed in the literature suggest testing relaxed forms of conditional independence. For instance,~\citet{gcm2020} propose the generalised covariance measure (GCM) which only characterises weak conditional dependence~\cite{daudin1980partial} and \citet{Zhang2017FeaturetoFeatureRF} propose a kernel-based test which focuses only on individual effects of the conditioning variable $Z$ on $X$ and $Y$. Some other tests are based on the knowledge of the conditional distributions in order to measure conditional dependencies. For example \citet{candes2018panning} assume that one has access to the exact conditional distributions,~\citet{BellotS19,shi2021double} approximate them using generative models and \citet{sen2017modelpowered} consider model-based methods to generate samples from the conditional distributions. In our work, we design a test statistic which characterizes the exact conditional independence of random variables and obtain its asymptotic null distribution without assuming any knowledge on the conditional distributions. Under some mild assumptions on the RKHSs considered, we also derive an approximate test statistic which admits the same asymptotic distribution and obtain a simple testing procedure from it.




% Although the application might seem immediate from~\citet{NEURIPS2019_0e2db0cb}, one need either to sample or estimate an expectation conditionally to $X$. While the option of sampling is impossible in practice, estimation of the conditional expectation can be done using Regularized Least Squares (RLS) as it was done by~\cite{zhang2012kernel}. Using this estimation in the test statistic asks convergence questions we tackle in this paper by a rigorous analysis using convergence rates of RLS from~\citep{fischer2017sobolev}. To ensure this convergence, assumptions on the spectrum and a source condition are required.  

% \textbf{Outline } We begin by providing the required background on kernels and conditional independence testing. Then, we turn to introduce a new statistic to measure conditional dependency. To avoid costful bootstrap or permutation procedures {\color{red}[what is the goal? to estimate what?]}, We also propose a normalized version of this statistic. Under some assumptions, we derive the asymptotic law of our statistics both for the null and alternative hypothesis. We also propose a random feature relaxation to accelerate the computation of the statistic. Finally, we use simulated data to validate our theoretical claims as well as to study the performance of our approach.




