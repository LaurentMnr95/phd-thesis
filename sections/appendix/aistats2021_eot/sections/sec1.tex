\section{Introduction}
% Optimal Transport (OT) has gained interest last years in machine learning with diverse applications in imaging~\citep{rabin2011wasserstein}, generative models~\citep{arjovsky2017wasserstein}, supervised learning~\citep{courty2016optimal}, or even adversarial examples~\citep{wong2019wasserstein}. The growing interest is also  due to easier computation thanks to  Sinkhorn algorithm~\citep{cuturi2013sinkhorn}. Although OT has a rich theoretical history~\citep{villani2003topics}, the  problem of the choice of the cost is a difficult and often arbitrary question and not treated in literatue. Choosing the ``right'' cost requires a strong knowledge about the problem. Indeed, one may ask whether the used cost is really relevant for the problem at hand. In this paper, we  introduce a new family of variational problems extending optimal transport, involving two or more costs which deals with such issues. 
%Here we propose a propose an envy-  % Given $N\geq 1$ cost functions, we are able to exactly determine locally which cost should be used in order to minimize the global objective while keeping robust to this local choice.\todo{change: robust to this local choice}

% Such a formulation generalizes the idea of Dudley metric where 


% we can choose locally between any arbirary cost

% define by generalizing the process under the construction behind the Dudley metric, we manage to build an extension of the OT problem where the quantity involves two or more generally a family of costs. %Recently,~\cite{paty2019subspace} proposed to project the distributions on a $k$-dimensional subspace that
% %maximizes their transport cost. In that sense, they aim to choose a cost among Mahalanobis square distances with kernels of rank $k$. In this paper, we consider a family of arbitrary costs. 

% One may ask whether the used cost is really relevant for the problem considered. Here, we define by generalizing the process under the construction behind the Dudley metric, we manage to build an extension of the OT problem where the quantity invo lves two or more generally a family of costs. %and tends to be robust against it while keeping trust in one of the cost if one is well adapted.
% %follow this line of research about choosing the right cost.  We come up with the question of the choice of the cost among $N\geq1$ arbitrary costs. 

% Optimal Transport (OT) has gained interest last years in machine learning with diverse applications in neuroimaging~\citep{janati2020multi}, generative models~\citep{arjovsky2017wasserstein,salimans2018improving}, supervised learning~\citep{courty2016optimal}, word embeddings \cite{alvarez2018towards}, reconstruction cell trajectories \citep{yang2020predicting,schiebinger2019optimal} or adversarial examples~\citep{wong2019wasserstein}. The key to use OT in these applications lies in the gain of computation efficiency thanks to regularizations that smoothes the OT problem. More specifically, when one uses an entropic penalty, one recovers the so called Sinkhorn distances~\citep{cuturi2013sinkhorn}. Although OT has a rich theoretical history~\citep{villani2003topics}, the choice of the cost is often a difficult question. Choosing the ``right'' cost requires a strong knowledge about the problem and one may ask whether the used cost to compare distributions is relevant for the problem at hand. In this paper, we introduce a new family of variational problems extending the optimal transport problem when multiple costs are involved. 
Optimal Transport (OT) has gained interest last years in machine learning with diverse applications in neuroimaging~\citep{janati2020multi}, generative models~\citep{arjovsky2017wasserstein,salimans2018improving}, supervised learning~\citep{courty2016optimal}, word embeddings \citep{alvarez2018towards}, reconstruction cell trajectories \citep{yang2020predicting,schiebinger2019optimal} or adversarial examples~\citep{wong2019wasserstein}. The key to use OT in these applications lies in the gain of computation efficiency thanks to regularizations that smoothes the OT problem. More specifically, when one uses an entropic penalty, one recovers the so called Sinkhorn distances~\citep{cuturi2013sinkhorn}. In this paper, we introduce a new family of variational problems extending the optimal transport problem when multiple costs are involved with various applications in fair division of goods/work and operations research problems. 

Fair division~\citep{steinhaus1949fair} has been widely studied by the artificial intelligence~\citep{lattimore2015linear} and economics~\citep{moulin2004fair} communities. Fair division consists in partitioning diverse resources among agents according to some fairness criteria. One of the standard problems in fair division is the fair cake-cutting problem~\citep{dubins1961cut,brandt2016handbook}. The cake is an heterogeneous resource, such as a cake with different toppings, and the agents have heterogeneous preferences over different parts of the cake, i.e., some people prefer the chocolate toppings, some prefer the cherries, others just want a piece as large as possible. Hence, taking into account these preferences, one might share the cake equitably between the agents. A generalization of this problem, for which achieving fairness constraints is more challenging, is when the splitting involves several heterogeneous cakes, and where the agents have linked preferences over the different parts of the cakes. This problem has many variants such as the cake-cutting with two cakes \citep{cloutier2010two}, or the Multi Type Resource Allocation~\citep{mackin2015allocating,wang2019multi}. In all these models it is assumed that there is only one indivisible unit per type of resource available in each cake, and once an agent choose it, he or she has to take it all. In this setting, the cake can be seen as a set where each element of the set represents a type of resource, for instance each element of the cake represents a topping. A natural relaxation of these problems is when a divisible quantity of each type of resources is available.
We introduce $\MOT$ (\textbf{E}quitable and \textbf{O}ptimal \textbf{T}ransport), a formulation that solves
both the cake-cutting and the cake-cutting with two cakes problems in this setting. 

Our problem expresses as an optimal transportation problem. Hence, we prove duality results and provide fast computation based on Sinkhorn algorithm. As interesting properties, some Integral Probability Metrics ($\textsc{IPM}$s)~\citep{muller1997integral} as Dudley metric~\citep{dudley1966weak}, or standard Wasserstein metric~\citep{villani2003topics} are particular cases of the $\MOT$ problem.
% Given two distributions and $N\geq 1$ cost functions, we present a new transportation problem where the goal is to partition among the costs the task of transporting one distribution towards an other, in order to minimize the global transporting cost while ensuring that every cost contributes equally to the transportation task. 
% % In that sense, we aim to get every cost involved equally in the transportation problem so that we can get the most out of it. 
% For instance, consider a government who has $N$ available shippers to transport its goods from one place to another and who tries to stimulate the economy after a pandemic outbreak. To do so, it may decide to pay its shippers equally for this task while still minimizing the global cost of transportation for the goods. We introduce $\MOT$ (\textbf{M}ultiple cost \textbf{O}ptimal \textbf{T}ransport), a formulation that solves this problem. We prove duality results and give interpretation for both primal and dual problems. $\MOT$ has strong links with notions of fairness and can be interpreted as  a criterion of fair division. Every shipper feels that their payoff is at least as good as the payoff of any other shipper, and thus no shipper feels envy~\cite{balcan2019envy}. As interesting properties, some Integral Probability Metrics ($\textsc{IPM}$s)~\citep{muller1997integral} as Dudley metric~\citep{dudley1966weak}, or standard Wasserstein metric~\citep{villani2003topics} are particular cases of the $\MOT$ problem.

% The problem as defined above, provides a principled answer to an open problem in optimal transport theory, and more importantly in its now numerous applications fields (e.g. imaging~\citep{rabin2011wasserstein}, generative models~\citep{arjovsky2017wasserstein}, supervised learning~\citep{courty2016optimal}, or even adversarial examples~\citep{wong2019wasserstein}). This problem is the one of choosing the most appropriate cost function to the problem at hand and its associated data space topology. 
% Indeed, although  OT has a rich theoretical history~\citep{villani2003topics}, this  problem has received little attention, and in the absence of a strong knowledge about the application domain, the choice of the cost function is rather arbitrary. Our formalism addresses this issue by considering a set of costs, and adaptively choosing the most locally appropriate one. For instance, if one has no knowledge of which cost to use for a given problem, $\MOT$ gives a good compromise to be robust to the selection of cost.


%Optimal Transport (OT) has gained interest last years in machine learning with diverse applications in imaging~\citep{rabin2011wasserstein}, generative models~\citep{arjovsky2017wasserstein}, supervised learning~\citep{courty2016optimal}, or even adversarial examples~\citep{wong2019wasserstein}. The growing interest is also  due to easier computation thanks to  Sinkhorn algorithm~\citep{cuturi2013sinkhorn}. Although OT has a rich theoretical history~\citep{villani2003topics}, the  problem of the choice of the cost is a difficult and often arbitrary question and not treated in literatue. Choosing the ``right'' cost requires a strong knowledge about the problem. Indeed, one may ask whether the used cost is really relevant for the problem at hand. In this paper, we  introduce a new family of variational problems extending optimal transport, involving two or more costs which deals with such issues. \todo{cite more recent litterature }

%We compute a dual formulation and propose an algorithm to compute it.

% Given $N\geq 1$ cost functions, we are able to exactly determine locally which cost should be used in order to minise to total transport cost of moving one distribution towards another one while having exactly the same costs for used for each cost functions. For example, assume that  you are at the head of a government that produces goods and you need to export them around the world. Moreover assume you have  access to many transport providers which propose different prices according to their locations. For example some regional providers may have very low prices to transport goods around their regions but expensive to transport outside. However to support all of providers, you wish to work with each of them, while paying a low global transport cost. Here is the problem: what is the cheapest strategy to follow to transport all your goods while giving exactly the same amount of money to each transport providers ? In this work, we answer this question by providing a new formulation of the transportation problem for multiple costs functions. 
% \todo{Illustration of the banana story with figure}

% \paragraph{Integral Probabilbity Metrics ($\textsc{IPM}$s)}
% Integral probability metrics are (semi-)metrics on the space of probability measures. We define for a set of functions $\mathcal{F}$ and two probability distributions $\mu$ and $\nu$: $\textsc{IPM}_\mathcal{F}(\mu,\nu)=\sup_{f\in\mathcal{F}}\int fd\mu-\int fd\nu$. They are defined with regards to the notion of weak convergence, i.e. $\mu_n\rightharpoonup \mu$ \emph{iff} for all $f$ continuous and bounded, $\int fd\mu_n\to \int fd\mu$. Conditions on $\mathcal{F}$ so that $\textsc{IPM}_\mathcal{F}$ metrizes weak convergence have been studied by~\cite{muller1997integral}. MMD metric, total variation distance, or the Dudley metric are standard and well studied examples of $\textsc{IPM}$s
% ~\citep{sriperumbudur2012empirical}.They recently regained interest in the Machine Learning community thanks to Generative Adversarial Networks (GANs)~\citep{goodfellow2014generative} where  $\textsc{IPM}$s are natural metrics  for  the discriminator~\citep{dziugaite2015training,arjovsky2017wasserstein}.  They also helped to build consistent two-sample tests~\cite{scetbon2019comparing}. So far, exact computation of $\textsc{IPM}$s and OT problems  between discrete distributions is often costly. For instance, exact computation of Dudley metric or Wasserstein-1 distances are Linear Programs whose resolution is usually polynomial. 

% \paragraph{Entropic Regularization.} As $\textsc{IPM}$s, the exact computation of Wasserstein distances in the discrete case (i.e. where distributions are sum of Diracs), defines a finite Linear Program, that might be costly to compute.
% ~\cite{cuturi2013sinkhorn} proposed to relax the OT problem with an an entropic regularization on the joint probability. With such a regularization, computation of OT distances are faster. Entropic regularization is based on Sinkhorn Algorithm CITE SINKHORN which enjoys a linear rate of convergence\todo{check here}. Computation implies matrix multiplications and hence, is suited to run fast on GPUs. Since this seminal work, the Sinkhorn iterations have been studied with links proximal gradient with $\KL$ operator and Bregman  iterative projections. Following this line of work, we propose an entropic regularisation to fasten the computation of the solution to  our problem\todo{lau:j'aime pas  ce que j'ai ecrit}

% \todo{Bullet points for contributions (which does in the same time the Outline)}
% \paragraph{Contributions.}  Given $N$ ground costs $\mathbf{c}:=(c_i)_{1\leq i\leq N}$, which one suits the best the transport problem locally on the ground space? How to select the most relevant one? Which mass should be transported with a cost $i$?
% In this paper, we introduce a new problem to tackle these questions. We study the problem from both theoretical and algorithmic point of view. We prove strong duality results in both continuous and discrete case. We also make the link with standard probability metrics. Following the line of work of~\cite{cuturi2013sinkhorn}, we provide a fast and efficient computation of the problem using entropic regularization. In Figure XXX, we illustrate our approach: we notice that costs are adapted  locally in the ground space, BLABLA TO COMPLETE


% \paragraph{Outline of the paper.} This work makes the following contributions. In Section~\ref{sec:got}, we introduce the multiple cost optimal transport problem and derive its dual formulation. We show that strong duality holds under mild assumptions. In Section~\ref{sec:properties}, we study the topologies induced on the space of probability distributions. In particular we recover a wide class of problems as standard Optimal Transport and the Holder metrics. In Section~\ref{sec:entropic}, we derive an entropic regularization for our problem, and a fast algorithm to compute the solution to the problem with discrete distributions. We round off the paper with numerical illustrations on both real and synthetic datasets to validate the approximation scheme.
\textbf{Contributions.} In this paper we introduce $\MOT$ an extension of Optimal Transport which aims at finding an equitable and optimal transportation strategy between multiple agents. We make the following contributions:
\begin{itemize}%[noitemsep,nosep,wide,partopsep=-4pt]\
    \item  In Section~\ref{sec:MOT}, we introduce the problem and show that it solves a fair division problem where heterogeneous resources have to be shared among multiple agents. We derive its dual and prove strong duality results. As a by-product, we show that $\MOT$ is related to some usual $\textsc{IPM}$s families and in particular the widely known Dudley metric.
    % derive its dual and prove strong duality results. We show that our problem solves a proportional and equitable resource allocation problem. Moreover, we show that the problem we defined is related to some usual $\textsc{IPM}$s families. As a by-product, we close the bridge between optimal transport and the Dudley metric. %and obtain as a consequence sufficient conditions on the cost functions for $\MOT$ to metrize the weak convergence.
    \item In Section~\ref{sec:entropic}, we propose an entropic regularized version of the problem, derive its dual formulation, obtain strong duality. We then provide an efficient algorithm to compute $\MOT$. Finally we propose other applications of $\MOT$ for Operations Research problems.
    %As a by-product, we obtain the differentiability of the regularized version and apply our approximation to compute barycenters between measures.
    % Then following the seminal work of~\cite{cuturi2013sinkhorn}, we propose a regularization to the problem by adding an entropic term (Section~\ref{sec:entropic}). We then provide algorithms based on this entropic regularization (Section~\ref{sec:algorithms}) and we illustrate $\MOT$ on both synthetic and real data to validate our claim. 
    
    % Finally in Sections~\ref{sec:entropic} and~\ref{sec:experiments}, we regularize our problem with an entropic term following the line of work of~\cite{cuturi2013sinkhorn}, and we lead experiments to validate our claim.
\end{itemize}
% Proofs are left in Appendix~\ref{sec:proofs}, along with discrete case computations in Appendix~\ref{sec:discrete}, other properties in Appendix~\ref{sec:other-pptys} and additional experiments.

% \paragraph{Contributions.} In this paper we introduce a new problem to handle multiple costs in optimal transport to answer our equality constraint among provider payoffs. After recalling  essential background from Optimal Transport theory, entropic regularization, and also $\textsc{IPM}$s in Section~\ref{sec:background}, we make the following contribution:
% \begin{enumerate}
%     \item We introduce a problem the following question: given $N$ shippers, and a transport objective, what is the cheapest transport while paying every single shippers the same amount of money? In Section~\ref{sec:got}, we answer positively by setting an adequate problem.  We provide duality results and interpretations for primal and dual problems. In Section~\ref{sec:properties}, we show mathematical and topological properties of the problem by making links with Wasserstein distance and $\textsc{IPM}$s.
%     \item Solving exactly this new problem corresponds to solve a Linear Program as in standard optimal transport problem. Then following the seminal work of~\cite{cuturi2013sinkhorn}, we propose a regularization to the problem by adding an entropic term (Section~\ref{sec:entropic}). We then provide algorithms based on this entropic regularization (Section~\ref{sec:algo}). Finally in Section~\ref{sec:experiments}, we lead experiments on both synthetic ans real data to validate our claim.
%     % Finally in Sections~\ref{sec:entropic} and~\ref{sec:experiments}, we regularize our problem with an entropic term following the line of work of~\cite{cuturi2013sinkhorn}, and we lead experiments to validate our claim.
% \end{enumerate}
% Additional details and proofs are left in supplementary material.

% One may ask whether the cost used when computing the OT distance is really relevant for the problem considered. Here by generalizing the process under the construction behind the dudley metric, we manage to build a generalization of the OT problem where the quantity involve two or more generally a family of costs and tends to be robust against it while keeping trust in one of the cost if one is well adapted.

% \begin{description}
% \item[C1.] At the extreme points of the spectrum, we have Wassertein  and Total Variation, a good compromise is the Dudley distance which enjoy the appealing property of allowing to locally interpolate between both. Two questions hold:
%     \begin{description}
%          \item[Q1] Can we generalize the idea behind the Dudley metric, that is to adapt locally by choosing the appropriate cost (i.e the finer one)?
%          \item[Q2] Can we come up with an efficient procedure to compute this generalized metric?
%     \end{description}
%     \begin{description}
%          \item[A1] We define a new family of variational problems generalizing Dudley and Wasserstein metrics , where we can choose locally between any arbirary cost - contrary to Dudley  where the interpolation is restricted to the trivial semi-metric and an arbitrary distance. A first contribution on which we built on the findings of our work is  to reformulate the Dudley distance as a supremum of the Wasserstein distance. More precisely this allowed to define a new set of constraints of the Dudley distance.
%           \item[A2] exact resolution of the primal problem, entropic relaxation, the problem is convex in its three variables, efficient coordinate descent algorithm 
%     \end{description}
% \item[C2.] Generalisation to an infinte set of costs.
% \end{description}

% This work makes the following contributions.
% \begin{itemize}
%     \item In section~\ref{sec:got}, we introduce a generalized formulation of the optimal transport problem and derive its dual formulation. We show that strong duality holds under mild assumptions.  
%     \item  In section~\ref{sec:properties}, we study the topologies induced on the space of probability distributions. In particular we recover a wide class of problems as standard Optimal Transport and the Holder metrics. 
%     \item In section~\ref{sec:entropic}, we derive  an entropic regularization for our problem, and a fast algorithm to compute the solution to the problem with discrete distributions. 
% \end{itemize}
% We round off the paper with numerical illustrations on both real and synthetic datasets
% to validate the approximation scheme.


% \paragraph{Outline of the paper} In the paper, we first recall some background in optimal transportation . We introduce a generalized problem of optimal transport, with providing primal and dual problems and proving strong duality under mild assumptions. We then lead a general study of our problem and show we recover a wide class of problems as standard Optimal Transport and Dudley metric. Then, we derive an entropic regularization for our problem, and a fast algorithm to compute the solution to the problem with discrete distributions. Finally, we run experiments on both real and synthetic datasets to validate the regularization.
    

%\section{Related work}
%\paragraph{Optimal Transport.} Optimal Transport (OT) is a long-standing and widely studied problem. OT is a infinite Linear optimization problem. The primal problem was first introduced as a mass transport problem by Monge, but the problem is not always feasible. Kantorovich refined the primal problem as an optimization of transference plan, proposed a dual formulation, and proved strong duality~\citep[Theorem 1.3]{villani2003topics}. Optimal transport computation was an issue until the seminal work of
%~\citep{cuturi2013sinkhorn} and the introduction of the entropic regularization. %Optimal transport as interesting in neural networks with AutoEncoders and Generra

% \paragraph{Integral Probabilbity Metrics ($\textsc{IPM}$s)}
% Integral probability metrics are (semi-)metrics on the space of probability measures. We define for a set of functions $\mathcal{F}$ and two probability distributions $\mu$ and $\nu$: $\textsc{IPM}_\mathcal{F}(\mu,\nu)=\sup_{f\in\mathcal{F}}\int fd\mu-\int fd\nu$. They are defined with regards to the notion of weak convergence, i.e. $\mu_n\rightharpoonup \mu$ \emph{iff} for all $f$ continuous and bounded, $\int fd\mu_n\to \int fd\mu$. Conditions on $\mathcal{F}$ so that $\textsc{IPM}_\mathcal{F}$ metrizes weak convergence have been studied by
% ~\cite{muller1997integral}. MMD metric, total variation distance, or the Dudley metric are standard and well studied examples of $\textsc{IPM}$s
% ~\citep{sriperumbudur2012empirical}.They recently regained interest in the Machine Learning community thanks to Generative Adversarial Networks (GANs)~\citep{goodfellow2014generative} where  $\textsc{IPM}$s are natural metrics  for  the discriminator~\citep{dziugaite2015training,arjovsky2017wasserstein}.  They also helped to build consistent two-sample tests~\cite{scetbon2019comparing}.So far, exact computation of $\textsc{IPM}$s and OT problems  between discrete distributions is often costly. For instance, exact computation of Dudley metric or Wasserstein-1 distances are Linear Programs whose resolution is usually polynomial. 

% \paragraph{Entropic Regularization.} As $\textsc{IPM}$s, the exact computation of Wasserstein distances in the discrete case (i.e. where distributions are sum of Diracs), defines a finite Linear Program, that might be costly to compute.
% ~\cite{cuturi2013sinkhorn} proposed to relax the OT problem with an an entropic regularization on the joint probability. With such a regularization, computation of OT distances are faster. Entropic regularization is based on Sinkhorn Algorithm CITE SINKHORN which enjoys a linear rate of convergence\todo{check here}. Computation implies matrix multiplications and hence, is suited to run fast on GPUs. Since this seminal work, the Sinkhorn iterations have been studied with links proximal gradient with $\KL$ operator and Bregman  iterative projections. Following this line of work, we propose an entropic regularisation to fasten the computation of the solution to  our problem\todo{lau:j'aime pas  ce que j'ai ecrit}




%\item[C1.] Variational formulation of the generalized Dudley distance (generalization in the sense we can consider any norms or cost). Interest : compute the Dudley efficiently. Why Dudley: Weaker than Wasserstein, take into account two norms. Interpolation between TV and Wasserstein. The very interest of our work, adapt locally to the problem at hand. Robustness wrt to the choice of the cost. Locally choose the best cost.


